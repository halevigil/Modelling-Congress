{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# history_df = pd.read_csv(\"data/US/2023-2024_118th_Congress/csv/history.csv\")\n",
    "# bills_df = pd.read_csv(\"data/US/2023-2024_118th_Congress/csv/bills.csv\")\n",
    "# bills_df_updated = bills_df.copy()\n",
    "# for row in history_df.itertuples():\n",
    "#   # print(row.action)\n",
    "#   match = re.search(r'^referred to the (house )?(committee|subcommittee)(.*?)(\\.|\\, and in addition)', row.action.lower())\n",
    "#   # print(match)\n",
    "#   if match:\n",
    "#     print((match.group(1) if match.group(1) else \"\")+match.group(2)+match.group(3))\n",
    "#     print(match.group(2))\n",
    "#     # bills_df_updated[\"committee\"]=match.group(2)\n",
    "\n",
    "\n",
    "\n",
    "# # bills_df[\"passed\"] = bills_df[]\n",
    "# # history_df['referred to committee'] = history_df['action'].str.lower().str.match(r'^referred to the (house committee|committee|house subcommittee|subcommittee)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all committee names\n",
    "import pandas as pd\n",
    "import re\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/bills.csv\" for term in range(111,120)]\n",
    "bills_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "committees=[committee for committee in set(bills_df[\"committee\"]) if type(committee)==str and committee !=\" \" and committee!=\"\"]\n",
    "committee_spellings={committee:[committee] for committee in committees}\n",
    "committee_search_item={}\n",
    "for committee in committees:\n",
    "  committee_spellings[committee]=[]\n",
    "  if not re.search(\"committee\",committee.lower()):\n",
    "    committee_spellings[committee].append(committee + \" Committee\")\n",
    "    committee_spellings[committee].append(\"Committee on \"+committee)\n",
    "    committee_spellings[committee].append(\"Committee on the \"+committee)\n",
    "    if re.search(\"Senate\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"Senate \",\"\",committee)+ \" Committee\")\n",
    "      committee_spellings[committee].append(\"Committee on \"+re.sub(\"Senate \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Senate Committee on the \"+re.sub(\"Senate \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"House Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on the \"+re.sub(\"Senate \",\"\",committee))\n",
    "    if re.search(\"House\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"House \",\"\",committee)+ \" Committee\")\n",
    "      committee_spellings[committee].append(\"Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"House Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"House Committee on the \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on the \"+re.sub(\"House \",\"\",committee))\n",
    "  committee_spellings[committee].append(committee)\n",
    "  if committee==\"House \" or committee==\"Senate \":\n",
    "    committee_search_item[committee]=committee\n",
    "  else:\n",
    "    if re.search(\"House \",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"House \",\"\",committee))\n",
    "      committee_search_item[committee]=re.sub(\"House \",\"\",committee)\n",
    "    elif re.search(\"Senate \",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"Senate \",\"\",committee))\n",
    "      committee_search_item[committee]=re.sub(\"Senate \",\"\",committee)\n",
    "    else:\n",
    "      committee_search_item[committee]=committee\n",
    "#   committees.append(\"Committee on \"+committee)\n",
    "\n",
    "# display(committee_spellings)\n",
    "# display(committee_search_item)\n",
    "# for committee in committees:\n",
    "#   print(committee)\n",
    "\n",
    "#   committees.append(\"Committee on \"+committee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "actions = list(set(history_df[\"action\"]))\n",
    "def actions_to_committees(actions):\n",
    "  action_committees_map={}\n",
    "  for action in actions:\n",
    "    action_committees_map[action]=[]\n",
    "    for committee in committees:\n",
    "      if re.search(committee_search_item[committee],action):\n",
    "        action_committees_map[action].append(committee)\n",
    "  return action_committees_map\n",
    "action_committees_map=actions_to_committees(actions)\n",
    "# display([x for x in action_committees_map.items() if len(x[1])>0])\n",
    "json.dump(action_committees_map,open(\"./outputs/action_committees_map.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_committees_map=json.load(open(\"./outputs/action_committees_map.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for comparing two actions to see similarity\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "# action_process_memo={}\n",
    "def topsort(g):\n",
    "  out=[]\n",
    "  visited=set()\n",
    "  def explore(node):\n",
    "    if node not in visited:\n",
    "      for neighbor in g[node]:\n",
    "        explore(neighbor)\n",
    "      out.append(node)\n",
    "      visited.add(node)\n",
    "  for node in g:\n",
    "    explore(node)\n",
    "  return list(reversed(out))\n",
    "def longest_path(g):\n",
    "  topsorting = topsort(g)\n",
    "  longest_path_node={}\n",
    "  for node in topsorting:\n",
    "    if node not in longest_path_node:\n",
    "      longest_path_node[node]=0\n",
    "    for neighbor in g[node]:\n",
    "      if neighbor not in longest_path_node:\n",
    "        longest_path_node[neighbor]=0\n",
    "      longest_path_node[neighbor]=max(longest_path_node[neighbor],longest_path_node[node]+1)\n",
    "  return max([longest_path_node[x] for x in longest_path_node]) if len(longest_path_node)!=0 else 0\n",
    "\n",
    "import itertools\n",
    "def process_action(action):\n",
    "  action=action.lower()\n",
    "  action=re.sub(\"committees\",\"committee\",action)\n",
    "  committees=[committee for committee in committee_search_item if re.search(committee_search_item[committee].lower(),action)]\n",
    "  if len(committees)>0:\n",
    "    committee_spellings_regex=\"|\".join([\"(\"+\")|(\".join(committee_spellings[committee])+\")\" for committee in committees]).lower()\n",
    "    action = re.sub(committee_spellings_regex,\"committee\",action)\n",
    "  action=re.sub(r\"\\,|\\.|\\-\",\" \",action)\n",
    "  action=re.sub(r\"[0-9]\",\"\",action)\n",
    "  action=re.sub(r\"  \",\" \",action)\n",
    "  action = re.sub(r'\\b(mr|mrs|ms|senator|representative) \\w+\\s', ' ',action)\n",
    "  action = re.sub(r'\\(.*?\\)', '',action)\n",
    "  action=re.sub(r\"  \",\" \",action)\n",
    "  return action\n",
    "action_process_map={}\n",
    "def process_split_action(action):\n",
    "  if action in action_process_map:\n",
    "    return action_process_map[action]\n",
    "  processed_action=process_action(action)\n",
    "  processed_split_action=processed_action.split(\" \")\n",
    "  processed_split_action=[x for x in processed_split_action if x!=\"\"]\n",
    "  action_process_map[action]=processed_split_action\n",
    "  return processed_split_action\n",
    "\n",
    "def longest_common_subsequence_old(sentence1,sentence2):\n",
    "  if len(sentence1)==0 or len(sentence2)==0:\n",
    "    return 0\n",
    "  matches=[]\n",
    "  for i1,word1 in enumerate(sentence1):\n",
    "    for i2,word2 in enumerate(sentence2):\n",
    "      if word1==word2:\n",
    "        matches.append((i1,i2))\n",
    "  if len(matches)==0:\n",
    "    return 0\n",
    "  g = {}\n",
    "  for match1 in matches:\n",
    "    if match1 not in g:\n",
    "      g[match1]=[]\n",
    "    for match2 in matches:\n",
    "      if match2 not in g:\n",
    "        g[match2]=[]\n",
    "      if match1[0]<match2[0] and match1[1]<match2[1]:\n",
    "        g[match1].append(match2)\n",
    "  return longest_path(g)+1\n",
    "def edit_distance(l1,l2):\n",
    "  dp=np.empty((len(l2)+1,len(l1)+1))\n",
    "  dp[0]=range(len(l1)+1)\n",
    "  dp[:,0]=range(len(l2)+1)\n",
    "  for i in range(1,len(l2)+1):\n",
    "    for j in range(1,len(l1)+1):\n",
    "      dp[i,j]=min(dp[i][j-1]+1,dp[i-1][j]+1,dp[i-1][j-1] if l1[j-1]==l2[i-1] else dp[i-1][j-1]+1)\n",
    "  return dp[-1,-1]\n",
    "def longest_common_subsequence(l1, l2):\n",
    "  # Helper function to calculate edit distance without substitutions allowed\n",
    "  def edit_distance_no_subs(l1, l2):\n",
    "    # Create dp matrix with size (len(l2)+1) x (len(l1)+1)\n",
    "    dp = np.empty((len(l2)+1, len(l1)+1))\n",
    "    # Initialize first row and column\n",
    "    dp[0] = range(len(l1)+1)\n",
    "    dp[:,0] = range(len(l2)+1)\n",
    "    # Set infinity value for substitutions\n",
    "    inf = float('inf')  # Use float('inf') instead of len(l1)*len(l2)\n",
    "    \n",
    "    # Fill dp matrix\n",
    "    for i in range(1, len(l2)+1):\n",
    "      for j in range(1, len(l1)+1):\n",
    "        # If characters match, take diagonal value\n",
    "        # If not, take minimum of insert/delete plus 1\n",
    "        # Substitutions are not allowed (inf cost)\n",
    "        dp[i,j] = min(\n",
    "          dp[i][j-1] + 1,  # deletion\n",
    "          dp[i-1][j] + 1,  # insertion\n",
    "          dp[i-1][j-1] if l1[j-1] == l2[i-1] else inf  # match or substitution\n",
    "        )\n",
    "    return dp[-1][-1]\n",
    "\n",
    "  # Handle empty sequences\n",
    "  if not l1 or not l2:\n",
    "    return 0\n",
    "    \n",
    "  # LCS length = (len(l1) + len(l2) - edit_distance_no_subs) / 2\n",
    "  # Since each non-matching character requires one insertion and one deletion\n",
    "  distance = edit_distance_no_subs(l1, l2)\n",
    "  return (len(l1) + len(l2) - distance) // 2\n",
    "\n",
    "def test_edit_distance():\n",
    "  # Test empty strings\n",
    "  assert edit_distance([], []) == 0\n",
    "  # Test one empty string\n",
    "  assert edit_distance([], ['a']) == 1\n",
    "  assert edit_distance(['a'], []) == 1\n",
    "  # Test single character difference\n",
    "  assert edit_distance(['a'], ['b']) == 1\n",
    "  # Test same strings\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'b', 'c']) == 0\n",
    "  # Test insertion\n",
    "  assert edit_distance(['a', 'c'], ['a', 'b', 'c']) == 1\n",
    "  # Test deletion\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'c']) == 1\n",
    "  # Test substitution\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'd', 'c']) == 1\n",
    "  # Test multiple operations\n",
    "  assert edit_distance(['a', 'b'], ['c', 'd']) == 2\n",
    "\n",
    "# test_edit_distance()\n",
    "mustmatch_regexes=[\"subcommittee\",\"committee\"]\n",
    "def similar_words(action1,action2,threshold):\n",
    "  for regex in mustmatch_regexes:\n",
    "    if not (bool(re.search(regex,action1.lower() if regex.islower() else action1))==bool(re.search(regex,action2.lower() if regex.islower() else action2))):\n",
    "      return False\n",
    "  action1=process_split_action(action1)\n",
    "  action2=process_split_action(action2)\n",
    "  length=max(len(action1),len(action2))\n",
    "  return abs(len(action1)-len(action2))<threshold*length and edit_distance(action1,action2)<threshold*length\n",
    "\n",
    "# Test cases\n",
    "# print(\"Test 1:\", subsequence_similarity(\"hi my name is Bob\", \"hi Bob\") == 2)  # Should match \"hi\" and \"Bob\"\n",
    "# print(\"Test 2:\", subsequence_similarity(\"a b c\", \"a c b\") == 2)  # Should match \"a\" and either \"b\" or \"c\"\n",
    "# print(\"Test 3:\", subsequence_similarity(\"x y z\", \"a b c\") == 0)  # No common subsequence except single words\n",
    "# print(\"Test 4:\", subsequence_similarity(\"\", \" \") == 0)  # Empty strings\n",
    "# print(\"Test 5:\", subsequence_similarity(\"hello world\", \"hello there world\") == 2)  # \"hello\" and \"world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similar_words(\"Committee on House Administration\",\"Committee on the Judiciary \",1/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to process all the actions ahead of time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "actions = list(set(history_df[\"action\"]))\n",
    "for action in actions:\n",
    "  process_split_action(action,action_process_map) # build up memo \n",
    "json.dump(action_process_map,open(\"./outputs/action_process_map.json\",\"w\"),indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_buckets_f(action):\n",
    "  if re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) in the nature of a substitute offered by\",action.lower()):\n",
    "    return \"H.Amdt. in the nature of a substitute offered by\"\n",
    "  if re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) offered by\",action.lower()):\n",
    "    return \"H.Amdt. offered by\"\n",
    "  if re.search(r\"^s\\.amdt\\.[0-9]*? amendment sa [0-9]*? proposed by\",action.lower()):\n",
    "    return \"S.Amdt. offered by\"\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket actions\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# actions=\n",
    "# actions = [sentence_split(action) for action in history_df[\"action\"]]\n",
    "\n",
    "\n",
    "def bucket_actions(similar,special_bucket_f = lambda x:None):\n",
    "  special_buckets=defaultdict(list)\n",
    "  buckets=[]\n",
    "  datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "  history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "  actions = sorted(list(set(history_df[\"action\"])))\n",
    "  for i,action in enumerate(actions):\n",
    "    action_special_bucket=special_bucket_f(action)\n",
    "    \n",
    "    bucketed=False\n",
    "    if action_special_bucket:\n",
    "      special_buckets[action_special_bucket] = action\n",
    "      bucketed=True\n",
    "    else:\n",
    "      for bucket in reversed(buckets):\n",
    "        if similar(bucket[0],action):\n",
    "          bucket.append(action)\n",
    "          bucketed=True\n",
    "          break\n",
    "    if i%1000==0:\n",
    "      print(i)\n",
    "    if not bucketed:\n",
    "      print(action)\n",
    "      buckets.append([action])\n",
    "  buckets.extend(special_buckets.values())\n",
    "  return buckets\n",
    "buckets=bucket_actions(lambda action1,action2:similar_words(action1,action2,1/4),special_bucket_f=special_buckets_f)\n",
    "  \n",
    "\n",
    "# buckets_raw=[]\n",
    "# special_buckets_raw=[[],[],[]]\n",
    "# actions = \n",
    "# for i,action in enumerate(actions):\n",
    "#   bucketed=False\n",
    "#   # if re.search(r\"^debate - the house proceeded with 10 minutes of debate on the .*?motion to recommit\",action[1].lower()):\n",
    "#   #   special_buckets_raw[3].append(action)\n",
    "#   #   continue\n",
    "#   for bucket in reversed(buckets_raw):\n",
    "#     # if action[0]==bucket[0][0]:\n",
    "#     #   bucket.append(action)\n",
    "#     #   bucketed=True\n",
    "#     #   break\n",
    "#     if abs(len(action[0])-len(bucket[0][0]))<1/3*max(len(bucket[0][0]),len(action[0])) and subsequence_similarity(action[0],bucket[0][0])>=max(len(action[0])*2/3, len(bucket[0][0])*2/3):\n",
    "#       # print(action)\n",
    "#       bucket.append(action)\n",
    "#       bucketed=True\n",
    "#       break\n",
    "# buckets_raw.extend(special_buckets_raw)\n",
    "# buckets = [[action[1] for action in bucket if type(action[1])==str] for bucket in buckets_raw]\n",
    "# display(buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save buckets\n",
    "import json\n",
    "from datetime import datetime\n",
    "with open(f\"./outputs/buckets{datetime.now().strftime(f\"%m-%d_%H:%M\")}.json\",\"w\") as file:\n",
    "  json.dump(buckets,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./outputs/buckets.json\",\"r\") as file:\n",
    "  buckets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_buckets(fs,buckets):\n",
    "  new_buckets=[[] for f in fs]\n",
    "  out=[]\n",
    "  for bucket in buckets:\n",
    "    # print(bucket)\n",
    "    for new_bucket,f in zip(new_buckets,fs):\n",
    "      if f(bucket):\n",
    "        new_bucket.extend(bucket)\n",
    "        moved=True\n",
    "        break\n",
    "    if not moved:\n",
    "        out.append(bucket)\n",
    "  out.extend(new_buckets)\n",
    "  return out\n",
    "def make_new_buckets(fs,buckets):\n",
    "  new_buckets=[[] for f in fs]\n",
    "  out=[]\n",
    "  for bucket in buckets:\n",
    "    out.append([])\n",
    "    for action in bucket:\n",
    "      # print(bucket)\n",
    "      moved=False\n",
    "      for new_bucket,f in zip(new_buckets,fs):\n",
    "        if f(bucket):\n",
    "          new_bucket.append(action)\n",
    "          moved=True\n",
    "          break\n",
    "      if not moved:\n",
    "        out[-1].append(action)\n",
    "  out.extend(new_buckets)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) in the nature of a substitute offered by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n",
    "\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) offered by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^s\\.amdt\\.[0-9]*? amendment sa [0-9]*? proposed by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_action_to_bucket_name={}\n",
    "for bucket in buckets:\n",
    "  for action in bucket:\n",
    "    specific_action_to_bucket_name[action]=bucket[0]\n",
    "# display(specific_action_to_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "def nums_to_bars(nums):\n",
    "  counter = Counter(nums)\n",
    "  keyvals=counter.items()\n",
    "  print(sorted(keyvals))\n",
    "\n",
    "  return [key for key,val in keyvals],[val for key,val in keyvals]\n",
    "lengths = [len(bucket) for bucket in buckets]\n",
    "xs,heights = nums_to_bars(lengths)\n",
    "ax.bar(xs,heights)\n",
    "ax.loglog()\n",
    "# ax.set_ybound(0,100)\n",
    "# ax.set_xbound(0,100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_names = [bucket[0] for bucket in buckets if type(bucket)==list and len(bucket)>0]\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "history_df[\"bucket\"]=history_df[\"action\"].apply(lambda action:specific_action_to_bucket_name[action])\n",
    "llm_buckets = []\n",
    "with open(\"./outputs/bucket_names_output.jsonl\", \"r\") as file:\n",
    "  for line in file:\n",
    "    if line.strip():  # Skip empty lines\n",
    "      response = json.loads(line)\n",
    "      llm_buckets.append(response[\"response\"][\"body\"][\"output\"][0][\"content\"][0][\"text\"])\n",
    "      \n",
    "history_df[\"llm_bucket\"] = history_df[\"bucket\"].map(dict(zip(bucket_names, llm_buckets)))\n",
    "\n",
    "# Add next bill_id column to compare\n",
    "history_df[\"next_bill_id\"]=history_df[\"bill_id\"].shift(-1)\n",
    "history_df[\"next_bucket\"]=history_df[\"bucket\"].shift(-1)\n",
    "history_df[\"next_bucket\"]=history_df.apply(lambda row:row[\"next_bucket\"] if row[\"next_bill_id\"]==row[\"bill_id\"] else None,1)\n",
    "history_df[\"next_bucket_llm\"]=history_df[\"llm_bucket\"].shift(-1)\n",
    "history_df[\"next_llm_bucket\"]=history_df.apply(lambda row:row[\"next_bucket_llm\"] if row[\"next_bill_id\"]==row[\"bill_id\"] else None,1)\n",
    "\n",
    "g={}\n",
    "for bucket1 in llm_buckets:\n",
    "  g[bucket1]={}\n",
    "  for bucket2 in llm_buckets:\n",
    "    g[bucket1][bucket2]=0\n",
    "  g[bucket1][None]=0\n",
    "for i, row in history_df.iterrows():\n",
    "  g[row[\"llm_bucket\"]][row[\"next_llm_bucket\"]]+=1\n",
    "# display(g)\n",
    "\n",
    "\n",
    "\n",
    "# display(history_df)\n",
    "# for i,row in history_df.iterrows():\n",
    "#   print(row.bucket)\n",
    "# for action in history_df:\n",
    "#   history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket1 in llm_buckets:\n",
    "  for bucket2 in llm_buckets:\n",
    "    if g[bucket1][bucket2]>200:\n",
    "      print(bucket1+\" -> \"+bucket2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch to come up with names for each bucket\n",
    "import random\n",
    "import numpy as np\n",
    "bucket_names_to_bucket={bucket[0]:bucket for bucket in buckets if type(bucket)==list and len(bucket)>0}\n",
    "manual_name_to_llm_reponse={}\n",
    "random.seed(1423)\n",
    "bucket_names_str=\"\\n\".join(random.sample(bucket_names,k=20))\n",
    "BATCHES=4\n",
    "bucket_names_batches=np.split(np.array(bucket_names),BATCHES)\n",
    "for batch_i in range(BATCHES):\n",
    "  input_len=0\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_input.jsonl\",\"w\") as file:\n",
    "    for i,bucket_name in enumerate(bucket_names_batches[batch_i]):\n",
    "      input=f\"\"\"I am analyzing a dataset full of Congressional actions. I have categorized these actions into buckets,\\\n",
    "      where each bucket has near-identical actions, usually differing only in the names/numbers of congresspeople,bills,votes,and committees. \\\n",
    "      I want to come up with names for these buckets. These names are usually just the actions with names/numbers of congresspeople, bills, votes and committees removed.\n",
    "      First, here is a sample of one action from the 20 buckets chosen at random:\\n\n",
    "      {bucket_names_str}\\n\n",
    "      Here are some example names you might give to bucket actions:\\n\n",
    "      Action from bucket: Mr. Whitfield moved that the Committee now rise.\\n\n",
    "      Bucket name: Congressperson moved the Committee now rise.\\n\\n\n",
    "      Action from bucket: The House resumed with the motion to agree in the Senate amendment to H.R. 83, with an amendment. (consideration: CR H9284-9290)\\n\n",
    "      Bucket name: The House resumed with the motion to agree in the Senate amendment to the bill, with an amendment.\\n\\n\n",
    "      Action from bucket: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee on Capital Markets and Government Sponsored Enterprises Prior to Introduction and Referral Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee on Capital Markets and Government Sponsored Enterprises\\n\n",
    "      Bucket name: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee Prior to Introduction and Referral. Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee\\n\\n\n",
    "      Action from bucket: Passed Senate with amendments by Yea-Nay. 93 - 7. Record Vote Number: 218.\\n\n",
    "      Bucket name: Passed Senate with amendments by Yea-Nay.\\n\\n\n",
    "      \n",
    "      Now, please come up with a name for the bucket with the following actions. Please only give me the name, nothing else in your response.\n",
    "      If there is only one line, it means the bucket has only one action:\\n\n",
    "      {\"\\n\".join(random.sample(bucket_names_to_bucket[bucket_name],k=20) if len(bucket_names_to_bucket[bucket_name])>20 else bucket_names_to_bucket[bucket_name])}\"\"\"\n",
    "    \n",
    "      input_len+=len(input)\n",
    "      json.dump({\"custom_id\":\"action\"+str(i),\"url\":\"/v1/responses\",\"method\":\"POST\",\"body\":{\"input\":input,\"model\":\"gpt-4.1-mini\"}}, file)\n",
    "      file.write(\"\\n\")\n",
    "    # manual_name_to_llm_reponse[bucket_name]=client.responses.create(input=input,model=\"gpt-4.1-mini\")\n",
    "    # print(bucket_name)\n",
    "  print(input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "import asyncio\n",
    "import json\n",
    "dotenv.load_dotenv()\n",
    "client = openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "for bucket_name in bucket_names[:5]:\n",
    "      input=f\"\"\"I am analyzing a dataset full of Congressional actions. I have categorized these actions into buckets,\\\n",
    "      where each bucket has near-identical actions, usually differing only in the names/numbers of congresspeople,bills,votes,and committees. \\\n",
    "      I want to come up with names for these buckets. These names are usually just the actions with names/numbers of congresspeople, bills, votes and committees removed.\n",
    "      First, here is a sample of one action from the 20 buckets chosen at random:\\n\n",
    "      {bucket_names_str}\\n\n",
    "      Here are some example names you might give to bucket actions:\\n\n",
    "      Action from bucket: Mr. Whitfield moved that the Committee now rise.\\n\n",
    "      Bucket name: Congressperson moved the Committee now rise.\\n\\n\n",
    "      Action from bucket: The House resumed with the motion to agree in the Senate amendment to H.R. 83, with an amendment. (consideration: CR H9284-9290)\\n\n",
    "      Bucket name: The House resumed with the motion to agree in the Senate amendment to the bill, with an amendment.\\n\\n\n",
    "      Action from bucket: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee on Capital Markets and Government Sponsored Enterprises Prior to Introduction and Referral Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee on Capital Markets and Government Sponsored Enterprises\\n\n",
    "      Bucket name: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee Prior to Introduction and Referral. Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee\\n\\n\n",
    "      Action from bucket: Passed Senate with amendments by Yea-Nay. 93 - 7. Record Vote Number: 218.\\n\n",
    "      Bucket name: Passed Senate with amendments by Yea-Nay.\\n\\n\n",
    "\n",
    "      Now, please come up with a name for the bucket with the following actions. Please only give me the name, nothing else in your response.\n",
    "      If there is only one line, it means the bucket has only one action:\\n\n",
    "      {\"\\n\".join(random.sample(bucket_names_to_bucket[bucket_name],k=20) if len(bucket_names_to_bucket[bucket_name])>20 else bucket_names_to_bucket[bucket_name])}\"\"\"\n",
    "      response=client.responses.create(input=input,model=\"gpt-4.1-mini\")\n",
    "      print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI client\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "client = openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "statuses=[batch.status for batch in client.batches.list()]\n",
    "while \"cancelling\" in statuses or \"in_progress\" in statuses:\n",
    "  time.sleep(60)\n",
    "  statuses=[batch.status for batch in client.batches.list()]\n",
    "print(\"starting batches\")\n",
    "for batch_i in range(3,4):\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_input.jsonl\",\"rb\") as file:\n",
    "    batch_file=client.files.create(file=file,purpose=\"batch\")\n",
    "  batch=client.batches.create(input_file_id=batch_file.id,endpoint=\"/v1/responses\",completion_window=\"24h\")\n",
    "  while batch.status!=\"completed\":\n",
    "    if batch.status==\"failed\":\n",
    "      break\n",
    "    time.sleep(60)\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_output.jsonl\",\"w\") as file:\n",
    "    file.write(client.files.content(file_id=client.batches.retrieve(batch_id=batch.id).output_file_id).text)\n",
    "  print(f\"finished batch {batch_i}\")\n",
    "  time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./outputs/bucket_names_batch3_output.jsonl\",\"w\") as file:\n",
    "  file.write(client.files.content(file_id=client.batches.retrieve(batch_id=\"batch_687e820d9b448190be9761b57efa014d\").output_file_id).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./outputs/bucket_names_output.jsonl\", \"w\") as outfile:\n",
    "  for batch_i in range(4):\n",
    "    with open(f\"./outputs/bucket_names_batch{batch_i}_output.jsonl\", \"r\") as infile:\n",
    "      outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.cancel(batch_id=batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.retrieve(batch_id=batch.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./outputs/buckets_manual3.json\",\"w\") as file:\n",
    "  json.dump(buckets_manual,file)\n",
    "  # for bucket in buckets:\n",
    "  #   file.write(str(bucket))\n",
    "  # file.writelines(buckets_manual)\n",
    "# display([bucket for bucket in sorted(buckets_manual, key=lambda x:len(x),reverse=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "bucket_name_to_bucket={bucket[0]:bucket for bucket in buckets_manual if type(bucket)==list and len(bucket)>0}\n",
    "for bucket in bucket_name_to_bucket:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(set(history_df[\"action\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_buckets=[[] for bucket in buckets]\n",
    "misclassified_by_subsequence=[]\n",
    "responses=[]\n",
    "for i in range(13,len(buckets)):\n",
    "  bruteforce_bucket=buckets[i]\n",
    "  print(i,bruteforce_bucket[0])\n",
    "  responses=[]\n",
    "  async with asyncio.TaskGroup() as tg:\n",
    "    for action in bruteforce_bucket:\n",
    "      input = \"These are two actions taken from a dataset of Congressional Actions. Please tell me if these two actions are the same except for potentially different names/numbers of committees, senators, representatives, bills, motions, or amendments, and potentially different vote records as well. Give me a one word answer \\\"Yes\\\" or \\\"No\\\":\\n\"\\\n",
    "        + action+\"\\n\"+bruteforce_bucket[0]\n",
    "      print(action)\n",
    "      responses.append((action,tg.create_task(client.responses.create(input=input,model=\"gpt-4.1-nano\"))))\n",
    "  for response in responses:\n",
    "    if response[1].result().output_text == \"Yes\":\n",
    "      llm_buckets[i].append(response[0])\n",
    "    else:\n",
    "      misclassified_by_subsequence.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "\n",
    "# bills_df_updated = bills_df.copy(   )\n",
    "actions=set()\n",
    "referred=set()\n",
    "ldict = {\"committee\":{},\"general\":[],\"Submitted in the Senate\":[],\"Introduced in House\":[]}\n",
    "subcommittee_dict={\"misc\":[]}\n",
    "committee_cats=[]\n",
    "\n",
    "\n",
    "committee_dict=defaultdict(list)\n",
    "# for regex in committee_cats:\n",
    "#   subcommittee_dict[regex]=[]\n",
    "committee_regexes = [\n",
    "  r'house rose',\n",
    "  r'committee rose',\n",
    "  r'moved that the committee rise',\n",
    "  r'committee rise agreed to by voice vote',\n",
    "  r'Reported \\(Amended\\) by',\n",
    "  r'submitted in the senate, considered, and agreed to',\n",
    "  r'motion by senator',\n",
    "  r'Committee of the Whole House on the state of the Union',\n",
    "  r'reported an original measure',\n",
    "  r'Ordered to be reported .*? favorably',\n",
    "  r'Original measure reported to Senate',\n",
    "  r'the speaker designated the honorable',\n",
    "  r'Granted an extension for further consideration',\n",
    "  r'motion to discharge committee filed',\n",
    "  r'withdrawn by unanimous consent',\n",
    "  r'asked unanimous consent',\n",
    "  r'motion to refer the bill',\n",
    "  r'the speaker appointed [conferees|additional conferees]',\n",
    "  r'filed written report',\n",
    "  r'motion to discharge senate committee',\n",
    "  r'the committee substitute as amended agreed to by unanimous consent',\n",
    "  r'the committee amendment as amended agreed to by unanimous consent',\n",
    "  r'supplemental report filed by',\n",
    "  r'Pursuant to clause 6\\(h\\) of rule XVIII, the Committee of the Whole resumed its sitting',\n",
    "  r'by direction of .*? and asked for its immediate consideration',\n",
    "  r'motion to discharge the committee on rules',\n",
    "  r'motion to table the motion',\n",
    "  r'reported adversely.*? by the committee',\n",
    "  r'rules committee resolution .*? reported to house',\n",
    "  r'on motion that the committee now rise agreed to .*?[by voice vote|without objection]',\n",
    "  r'failed to report favorably',\n",
    "  r'ordered to be reported favorably',\n",
    "  r'The resolution provides? that.*? amendment in the nature of a substitute.*? shall be considered as adopted',\n",
    "  r'committee amendments?.*? agreed to by unanimous consent',\n",
    "  r'moved to commit to the committee',\n",
    "  r'committee agreed to seek consideration under suspension of the rules',\n",
    "  r'all points of order against consideration .*? are waived',\n",
    "  \"committee consideration and mark-up session held\",\n",
    "  \"hearings held\",\n",
    "  r\"committee.*? discharged\",\n",
    "  r\"reported by.*?with.*?report\",\n",
    "  r\"reported by.*?without.*?report\",\n",
    "  \"forwarded by subcommittee to full committee.*? by voice vote\",\n",
    "  \"forwarded by subcommittee to full committee.*? by unanimous consent\",\n",
    "  \"forwarded by subcommittee to full committee.*? by the yeas and nays\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by the yeas and nays\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by voice vote\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by unanimous consent\",\n",
    "  r\"amendment.*? offered by\",\n",
    "  \"it shall be in order to consider as an original bill for the purpose of amendment\",\n",
    "  \"moved to recommit\",\n",
    "  \"moved that the committee now rise\",\n",
    "  r\"[read twice and referred to]|[read the second time and referred to]\",\n",
    "  r\"^reported by\",\n",
    "  r\"amdt.*?referred to\",\n",
    "  r\"amendment.*? proposed by\",\n",
    "  r\"the committee resumed? it\\'?s sitting\",\n",
    "  r\"by[| the] direction of the committee on rules,.*? called up\",\n",
    "  \"rule provides for consideration of\",\n",
    "  \"resolution provides for consideration of\",\n",
    "  r\"^referred\",\n",
    "  r\"^re-referred\",\n",
    "  \"^received in the senate and referred\"\n",
    "]\n",
    "for row in history_df.itertuples():\n",
    "  # print(row.action)\n",
    "  # match = re.match(r'^referred to the (house )?(committee|subcommittee)', row.action.lower())\n",
    "  categorized=False\n",
    "  if re.search(\"committee\",row.action.lower()) and not re.search(\"committee of the whole\",row.action.lower()):\n",
    "    for regex in committee_regexes:\n",
    "      if re.search(regex,row.action.lower() if regex.islower() else row.action):\n",
    "        committee_dict[regex].append(row.action)\n",
    "        categorized=True\n",
    "        break\n",
    "    if not categorized:\n",
    "      committee_dict[\"misc\"].append(row.action)\n",
    "  elif re.search(\"Submitted in the Senate\",action):\n",
    "    ldict[\"Submitted in the Senate\"].append(action)\n",
    "  elif re.match(r\"Introduced in House$\",action):\n",
    "    ldict[\"Introduced in House\"].append(action)\n",
    "  else:\n",
    "    ldict[\"general\"].append(action)\n",
    "# committee_dict=dict(committee_dict)\n",
    "display(sorted(committee_dict[\"misc\"]))\n",
    "# display(ldict[\"general\"])\n",
    "# display(ldict[\"committee\"])\n",
    "# for key in ldict:\n",
    "#   display(ldict[key])\n",
    "# display(ldict)\n",
    "# for key in types:\n",
    "#   print(key,types[key])\n",
    "\n",
    "\n",
    "# bills_df[\"passed\"] = bills_df[]\n",
    "# history_df['referred to committee'] = history_df['action'].str.lower().str.match(r'^referred to the (house committee|committee|house subcommittee|subcommittee)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not re.search(r\"general debate\",\"GENERAL DEBATE - The Committee of the Whole proceeded with one hour of general debate on H.R. 277.\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Reported (Amended) by\" in \"Reported (Amended) by the Committee on Natural Resources. H. Rept. 118-919.\":\n",
    "  print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(\"embedding_batch.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.create(input_file_id=batch_input_file.id,endpoint=\"/v1/outputs\",completion_window=\"24h\",metadata={\"description\":\"nightly eval job\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in client.files.content(\"file-Nybj7TyzxjuL7NXTWsNqGw\").text.split(\"\\n\"):\n",
    "  print(json.loads(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./outputs/bucket_names_batch0_output.jsonl\",\"w\") as file:\n",
    "  file.write(client.files.content(\"file-Nybj7TyzxjuL7NXTWsNqGw\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list(client.files.retr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(client.batches.retrieve(\"batch_685462b110ac8190bb51093fa9c12910\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = client.files.content(\"file-Vt5fNxGnSo2qh796HC27PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_response = client.files.content(\"file-Vt5fNxGnSo2qh796HC27PC\")\n",
    "# with open(\"outputs/2025-26-history.jsonl\",\"w\") as f:\n",
    "#   f.write(file_response.text)\n",
    "json_file = pd.read_json(\"outputs/2025-26-history.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "\n",
    "print(json_file.iloc[0])\n",
    "df = pd.DataFrame()\n",
    "df[\"bill#\"]=json_file[\"custom_id\"]\n",
    "# df[\"bill#\"]=json_file.apply(lambda x:x[\"custom_id\"])\n",
    "df[\"embedding\"]=json_file[\"response\"].apply(lambda x: x[\"body\"][\"data\"][0][\"embedding\"])\n",
    "df.sort_values(by=\"bill#\")\n",
    "outputs_array = np.array(df[\"embedding\"].tolist())\n",
    "# tsne_result = sklearn.manifold.TSNE().fit_transform(embeddings_array)\n",
    "# display(df)\n",
    "# display\n",
    "# (json_file[\"response\"].iloc[0])\n",
    "# display(type(json_file[\"custom_id\"]))\n",
    "# json_file.iloc[0][\"response\"][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_df = pd.read_csv(\"data/US/2025-2026_119th_Congress/csv/history.csv\")\n",
    "def bucket(cluster_per_action):\n",
    "  clusters={}\n",
    "  for i,cluster in enumerate(cluster_per_action):\n",
    "    cluster=int(cluster)\n",
    "    if cluster not in clusters:\n",
    "      clusters[cluster]=set()\n",
    "    clusters[cluster].add(history_df.iloc[i][\"action\"])\n",
    "  return list(clusters.values())\n",
    "def unique_actions(buckets):\n",
    "  buckets_unique=[]\n",
    "  for bucket in buckets:\n",
    "    bucket_unique={re.sub(\"-\",\"\",re.sub(r'\\d+', '', x)) for x in bucket}\n",
    "    buckets_unique.append(list(bucket_unique))\n",
    "  return buckets_unique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optics_result = sklearn.cluster.OPTICS().fit_predict(embeddings_array)\n",
    "# optics_buckets = unique_actions(bucket(optics_result))\n",
    "# display(optics_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "topleft_pts=[i for i,x in enumerate(tsne_result) if x[0]<-90 and x[1]>0]\n",
    "input_file=[json.loads(x)[\"body\"][\"input\"] for x in open(\"embedding_batch.jsonl\", \"rb\")]\n",
    "\n",
    "def plot_2d(arr):\n",
    "  plt.scatter([x[0] for x in arr],[x[1] for x in arr])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "intro_house=[i for i in range(len(input_file)) if input_file[i]==\"Introduced in House\"]\n",
    "# np.mean()\n",
    "means_overall=np.mean(embeddings_array,axis=0)\n",
    "std_overall = np.std(embeddings_array,axis=0)\n",
    "means_house=np.mean(embeddings_array[intro_house],axis=0)\n",
    "std_house = np.std(embeddings_array[intro_house],axis=0)\n",
    "print(np.mean(std_overall))\n",
    "print(np.mean(std_house))\n",
    "# fig, axs = plt.subplots(2)\n",
    "# axs[0].bar(range(len(means_overall)),means_overall-means_house)\n",
    "# ax[1].bar(range(len(means_overall)),std_overall)\n",
    "# display(np.std(embeddings_array[intro_house]))\n",
    "# display([input_file[x] for x in topleft_pts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO05QBEL1BiZJV8jZLZO8J/",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
