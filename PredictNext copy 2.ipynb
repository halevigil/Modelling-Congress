{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"outputs/buckets_07-29_charsim17.json\",\"r\") as file:\n",
    "  buckets = json.load(file)\n",
    "bucket_map={}\n",
    "for name in buckets:\n",
    "  for action in buckets[name]:\n",
    "    bucket_map[action]=name\n",
    "common_bucket_names = [name for name in buckets.keys() if len(buckets[name])>=10]\n",
    "common_bucket_names_inv={name:i for i,name in enumerate(common_bucket_names)}\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df_by_term=[pd.read_csv(ds) for ds in datasets]\n",
    "for i,df in enumerate(history_df_by_term):\n",
    "  df[\"term\"]=i+111\n",
    "history_df=pd.concat(history_df_by_term)\n",
    "# with open(\"./outputs/bucket_names_output.jsonl\", \"r\") as file:\n",
    "#   for line in file:\n",
    "#     if line.strip():  # Skip empty lines\n",
    "#       response = json.loads(line)\n",
    "#       llm_buckets.append(response[\"response\"][\"body\"][\"output\"][0][\"content\"][0][\"text\"])\n",
    "      \n",
    "# history_df[\"llm_bucket\"] = history_df[\"bucket\"].map(dict(zip(bucket_names, llm_buckets)))\n",
    "\n",
    "# Add next bill_id column to compare\n",
    "history_df[\"bucket\"]=history_df[\"action\"].apply(lambda action:bucket_map[action])\n",
    "bills={bill_id:group for (bill_id,group) in history_df.groupby(\"bill_id\")}\n",
    "bill_ids = list(bills.keys())\n",
    "\n",
    "MIN_TERM=min(history_df[\"term\"])\n",
    "MAX_TERM=max(history_df[\"term\"])\n",
    "N_TERMS=MAX_TERM-MIN_TERM+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(buckets.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(common_bucket_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha=2/3\n",
    "data=[]\n",
    "\n",
    "bill_id=history_df.iloc[0][\"bill_id\"]\n",
    "predecessors_vec=np.zeros(len(common_bucket_names))\n",
    "predecessor_vec=np.zeros(len(common_bucket_names))\n",
    "one_hot_curr = np.zeros(len(common_bucket_names))\n",
    "inputs=[]\n",
    "outputs=[]\n",
    "n_predecessors_prev=0\n",
    "for i,row in history_df.iterrows():\n",
    "  one_hot_prev = np.array(one_hot_curr)\n",
    "  one_hot_curr = np.zeros(len(common_bucket_names))\n",
    "  if row.bucket in common_bucket_names_inv:\n",
    "    one_hot_curr[common_bucket_names_inv[row.bucket]]=1\n",
    "\n",
    "  # if sum(one_hot_prev)!=1 and sum(one_hot_prev)!=0:\n",
    "  #   print(\"non-0/1 sum\",i)\n",
    "  # n_predecessors=np.sum(predecessors_vec)\n",
    "  # if n_predecessors!=0 and n_predecessors!=n_predecessors_prev+1 and n_predecessors!=n_predecessors_prev:\n",
    "  #   print(n_predecessors,n_predecessors_prev,i)\n",
    "  # n_predecessors_prev=n_predecessors\n",
    "  if row[\"bill_id\"]!=bill_id:\n",
    "    bill_id=row[\"bill_id\"]\n",
    "    one_hot_prev=np.zeros(len(common_bucket_names))\n",
    "    predecessors_vec=np.zeros(len(common_bucket_names))\n",
    "    predecessor_vec=np.zeros(len(common_bucket_names))\n",
    "  predecessor_vec=(1-alpha)*one_hot_prev+alpha*np.array(predecessor_vec)\n",
    "  predecessors_vec=np.array(predecessors_vec)+one_hot_prev\n",
    "  one_hot_term=np.zeros(N_TERMS)\n",
    "  one_hot_term[row.term-MIN_TERM]=1\n",
    "  # input_vec = np.concat([predecessor_vec,predecessors_vec,one_hot_term])\n",
    "  entry={\"predecessor\":predecessor_vec,\"predecessors\":predecessors_vec,\"term\":one_hot_term,\"output_vec\":one_hot_curr,\"output\":common_bucket_names_inv[row.bucket] if row.bucket in common_bucket_names_inv else -1}\n",
    "  # print(entry)\n",
    "  data.append(entry)\n",
    "\n",
    "# print(data[0])\n",
    "# inputs=np.stack(inputs)\n",
    "# outputs=np.stack(outputs)\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/input\",inputs[0])\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/output\",outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"outputs/prediction_vecs_07-29_charsim17.npz\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"outputs/prediction_vecs_07-29_charsim17.npz\",allow_pickle=True)[\"arr_0\"]\n",
    "display(data[0])\n",
    "# display(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "  def __init__(self,path):\n",
    "    self.data = np.load(path,allow_pickle=True)[\"arr_0\"]\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "  def __getitem__(self,idx):\n",
    "    entry = self.data[idx]\n",
    "    input = np.concatenate([entry[\"predecessor\"],entry[\"predecessors\"],entry[\"term\"]])\n",
    "    return input,entry[\"output_vec\"]\n",
    "\n",
    "ds = ActionDataset(\"outputs/prediction_vecs_07-29_charsim17.npz\")\n",
    "# print(len(ds))\n",
    "train_dataset,test_dataset,val_dataset = torch.utils.data.random_split(ds,[0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arr(arr):\n",
    "  for line in arr:\n",
    "    print([float(x) for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# class ActionModel(torch.nn.Module):\n",
    "#   def __init__(self,input_len,output_len):\n",
    "#     self.linear= torch.nn.Linear\n",
    "#     self.input_len=input_len\n",
    "#     self.output_len = output_len\n",
    "#   def forward(self,input):\n",
    "# class ScalarMultiply(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     A PyTorch module that multiplies input by a learnable scalar parameter.\n",
    "    \n",
    "#     Args:\n",
    "#         initial_value (float): Initial value for the scalar parameter. Default: 1.0\n",
    "#         requires_grad (bool): Whether the parameter should be trainable. Default: True\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, initial_value=1.0, requires_grad=True):\n",
    "#         super(ScalarMultiply, self).__init__()\n",
    "#         self.scalar = torch.nn.Parameter(\n",
    "#             torch.tensor(initial_value, dtype=torch.float32), \n",
    "#             requires_grad=requires_grad\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass: multiply input by the learnable scalar.\n",
    "        \n",
    "#         Args:\n",
    "#             x (torch.Tensor): Input tensor of any shape\n",
    "            \n",
    "#         Returns:\n",
    "#             torch.Tensor: Input multiplied by the scalar, same shape as input\n",
    "#         \"\"\"\n",
    "#         return x * self.scalar\n",
    "    \n",
    "#     def extra_repr(self):\n",
    "#         \"\"\"String representation for print/debugging\"\"\"\n",
    "#         return f'scalar={self.scalar.item():.4f}'\n",
    "\n",
    "\n",
    "\n",
    "# hyperparam_sets=[{\"folder\":\"outputs/models/07-30_lr1e-4beta.01\"}]\n",
    "# for hyperparams in hyperparam_sets:\n",
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "\n",
    "folder=\"outputs/models/07-30_lr1e-4_beta10\"\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+N_TERMS,len(common_bucket_names))\n",
    "# model.load_state_dict(torch.load(\"outputs/models/07-30_lr1e-5_beta.1/epoch70.pt\"))\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "pred_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "beta=10\n",
    "epsilon=1e-8\n",
    "# folder = hyperparams[\"folder\"]\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "def reg_loss_fn(weights):\n",
    "   return torch.sum(torch.abs(weights))\n",
    "  #  return torch.sum(torch.sqrt(torch.abs(weights)+epsilon))\n",
    "for epoch in range(100):\n",
    "  for i,(input,output) in enumerate(train_loader):\n",
    "    optim.zero_grad()\n",
    "    input=input.float()\n",
    "    output=output.float()\n",
    "    pred = model(input)\n",
    "    pred_loss = pred_loss_fn(pred,output)\n",
    "    reg_loss = reg_loss_fn(model.weight)\n",
    "    # print(\"pred loss:\",pred_loss)\n",
    "    # print(\"reg loss:\",reg_loss)\n",
    "    loss = pred_loss+beta*reg_loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "  print(f\"finished epoch {epoch}\")\n",
    "  \n",
    "  torch.save(model.state_dict(),folder+f\"/epoch{epoch}.pt\")\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    val_pred_loss=torch.scalar_tensor(0)\n",
    "    val_reg_loss=reg_loss_fn(model.weight)\n",
    "    for input,output in val_loader:\n",
    "      input=input.float()\n",
    "      output=output.float()\n",
    "      pred = model(input)\n",
    "      # for line in output:\n",
    "      #   for i,x in enumerate(line):\n",
    "      #     if x>=0.9:\n",
    "      #       print(i)\n",
    "      #   print(torch.sum(line))\n",
    "      # break\n",
    "      # print_arr(output.numpy())\n",
    "      # print(\"sum:\",torch.sum(output))\n",
    "      val_pred_loss+=pred_loss_fn(pred,output)\n",
    "    val_pred_loss/=len(val_loader)\n",
    "    with open(folder+f\"/losses_epoch{epoch}\",\"w\") as file:\n",
    "        file.write(\"val pred loss: \"+str(float(val_pred_loss))+\". val ridge loss: \"+str(float(val_reg_loss)))\n",
    "    print(\"val pred loss:\",val_pred_loss,\"val reg loss:\",val_reg_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+N_TERMS,len(common_bucket_names))\n",
    "# model.load_state_dict(torch.load(\"outputs/models/07-30_lr1e-5_beta.1/epoch70.pt\"))\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "with torch.no_grad():\n",
    "  val_pred_loss=torch.scalar_tensor(0)\n",
    "  val_ridge_loss=torch.sum(torch.abs(model[0].weight))\n",
    "  for input,output in val_loader:\n",
    "    input=input.float()\n",
    "    output=output.float()\n",
    "    pred = model(input)\n",
    "    for line in pred:\n",
    "      print(list(line))\n",
    "      # for i,x in enumerate(line):\n",
    "      #   if x>=0.1:\n",
    "      #     print(i,float(x),end=\"\")\n",
    "      # print()\n",
    "      print(torch.sum(line))\n",
    "    # break\n",
    "    # print_arr(output.numpy())\n",
    "    # print(\"sum:\",torch.sum(output))\n",
    "  # val_pred_loss/=len(val_loader)\n",
    "  print(\"val pred loss:\",val_pred_loss,\"val ridge loss:\",val_ridge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buckets[\"H.AMDT.1 Amendment (A001) offered by Mr. Becerra. (consideration: CR H6; text: CR H6)Amendment nominated Democrat candidates for Officers of the House.\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+N_TERMS,len(common_bucket_names))\n",
    "model.load_state_dict(torch.load(\"outputs/models/07-30_lr1e-5_beta1/epoch79.pt\"))\n",
    "weights = model.weight\n",
    "weights=weights.detach().numpy()\n",
    "print(weights.shape)\n",
    "pred_connections=[]\n",
    "preds_connections=[]\n",
    "term_connections=[]\n",
    "# term_params=[dict() for i in range(N_TERMS)]\n",
    "for i,bucket1 in enumerate(common_bucket_names):\n",
    "  for j,bucket2 in enumerate(common_bucket_names):\n",
    "    if bucket1==bucket2:\n",
    "      continue\n",
    "    pred_connections.append((float(weights[j][i]),bucket1,bucket2))\n",
    "    preds_connections.append(( float(weights[j][i+len(common_bucket_names)]),bucket1,bucket2))\n",
    "for term_i in range(N_TERMS):\n",
    "  for j,bucket2 in enumerate(common_bucket_names):\n",
    "    term_connections.append(( float(weights[j][term_i+2*len(common_bucket_names)]),bucket1,bucket2))\n",
    "\n",
    "pred_connections.sort(reverse=True, key = lambda x:abs(x[0]))\n",
    "preds_connections.sort(reverse=True,key = lambda x:abs(x[0]))\n",
    "term_connections.sort(reverse=True,key = lambda x:abs(x[0]))\n",
    "for connection in pred_connections[:100]:\n",
    "  print(connection[0],connection[1])\n",
    "# for connection in preds_connections[:500]:\n",
    "#   print(connection[0], connection[1])\n",
    "# for connection in preds_connections[-500:]:\n",
    "#   print(connection[0], connection[1])\n",
    "# display(\"predecessor:\",pred_connections)\n",
    "# display(\"predecessors:\",preds_connections)\n",
    "# display(\"terms:\",term_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_connections=[]\n",
    "preds_connections=[]\n",
    "for bucket1 in common_bucket_names:\n",
    "  for bucket2 in common_bucket_names:\n",
    "    pred_connections.append((predecessor_params[bucket1][bucket2],bucket1 +\" -> \"+bucket2))\n",
    "    preds_connections.append((predecessors_params[bucket1][bucket2],bucket1 +\" -> \"+bucket2))\n",
    "for term_i in range(N_TERMS):\n",
    "  for bucket2 in common_bucket_names:\n",
    "    pred_connections.append((predecessor_params[bucket1][bucket2],bucket1 +\" -> \"+bucket2))\n",
    "    preds_connections.append((predecessors_params[bucket1][bucket2],bucket1 +\" -> \"+bucket2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "import copy\n",
    "import random\n",
    "model = sklearn.linear_model.LogisticRegression(penalty=\"l1\",solver=\"saga\",multi_class=\"multinomial\")\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "data_shuffled = copy.deepcopy(data)\n",
    "random.shuffle(data_shuffled)\n",
    "inputs = [np.concatenate([e[\"predecessors\"],e[\"predecessor\"],e[\"term\"]]) for e in data_shuffled]\n",
    "outputs = [e[\"output\"] for e in data_shuffled]\n",
    "inputs= scaler.fit_transform(inputs)\n",
    "divider = int(len(data)*0.8)\n",
    "train_inputs,test_inputs = inputs[:divider], data_shuffled[divider:]\n",
    "train_outputs,test_outputs = outputs[:divider], outputs[divider:]\n",
    "model.fit(train_inputs,train_outputs)\n",
    "test_pred = model.predict_proba(test_inputs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billanalysis-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
