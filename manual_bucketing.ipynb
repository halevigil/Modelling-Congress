{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all committee names\n",
    "import pandas as pd\n",
    "import re\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/bills.csv\" for term in range(111,120)]\n",
    "bills_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "committees=[committee for committee in set(bills_df[\"committee\"]) if type(committee)==str and committee !=\" \" and committee!=\"\"]\n",
    "committee_spellings={committee:[committee] for committee in committees}\n",
    "committee_search_item={}\n",
    "for committee in committees:\n",
    "  committee_spellings[committee]=[]\n",
    "  if not re.search(\"committee\",committee.lower()):\n",
    "    committee_spellings[committee].append(committee + \" Committee\")\n",
    "    committee_spellings[committee].append(\"Committee on \"+committee)\n",
    "    committee_spellings[committee].append(\"Committee on the \"+committee)\n",
    "    if re.search(\"Senate\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"Senate \",\"\",committee)+ \" Committee\")\n",
    "      committee_spellings[committee].append(\"Senate Committee on the \"+re.sub(\"Senate \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Senate Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on the \"+re.sub(\"Senate \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on \"+re.sub(\"Senate \",\"\",committee))\n",
    "    if re.search(\"House\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"House \",\"\",committee)+ \" Committee\")\n",
    "      committee_spellings[committee].append(\"House Committee on the \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"House Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on the \"+re.sub(\"House \",\"\",committee))\n",
    "      committee_spellings[committee].append(\"Committee on \"+re.sub(\"House \",\"\",committee))\n",
    "  committee_spellings[committee].append(committee)\n",
    "  if re.search(\"(House )|(Senate )\",committee):\n",
    "    committee_spellings[committee].append(re.sub(\"(House )|(Senate )\",\"\",committee))\n",
    "  if re.search(\"committee\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"(Subcommittee on )|(Subcommittee for )|(Subcommittee )|( Subcommittee)|(Committee on )|(Committee for )|( Committee)|(Committee )\",\"\",committee))\n",
    "  if re.search(\"committee\",committee) and re.search(\"(House )|(Senate )\",committee):\n",
    "      committee_spellings[committee].append(re.sub(\"(Subcommittee on )|(Subcommittee for )|(Subcommittee )|( Subcommittee)|(Committee on )|(Committee for )|( Committee)|(Committee )|(Senate )|(House )\",\"\",committee))\n",
    "  committee_search_item[committee]=re.sub(\"(Subcommittee on )|(Subcommittee for )|(Subcommittee )|( Subcommittee)|(Committee on )|(Committee for )|( Committee)|(Committee )|(Senate )|(House )\",\"\",committee)\n",
    "#   committees.append(\"Committee on \"+committee)\n",
    "\n",
    "display(committee_spellings)\n",
    "display(committee_search_item)\n",
    "# for committee in committees:\n",
    "#   print(committee)\n",
    "\n",
    "#   committees.append(\"Committee on \"+committee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each action to the committee[s] it is in\n",
    "import json\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "actions = list(set(history_df[\"action\"]))\n",
    "def actions_to_committees(actions):\n",
    "  action_committees_map={}\n",
    "  for action in actions:\n",
    "    action_committees_map[action]=[]\n",
    "    for committee in committees:\n",
    "      if re.search(committee_search_item[committee],action):\n",
    "        action_committees_map[action].append(committee)\n",
    "  return action_committees_map\n",
    "action_committees_map=actions_to_committees(actions)\n",
    "# display([x for x in action_committees_map.items() if len(x[1])>0])\n",
    "json.dump(action_committees_map,open(\"./outputs/action_committees_map.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "action_committees_map=json.load(open(\"./outputs/action_committees_map.json\",\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for comparing two actions to see similarity\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "# action_process_memo={}\n",
    "def topsort(g):\n",
    "  out=[]\n",
    "  visited=set()\n",
    "  def explore(node):\n",
    "    if node not in visited:\n",
    "      for neighbor in g[node]:\n",
    "        explore(neighbor)\n",
    "      out.append(node)\n",
    "      visited.add(node)\n",
    "  for node in g:\n",
    "    explore(node)\n",
    "  return list(reversed(out))\n",
    "def longest_path(g):\n",
    "  topsorting = topsort(g)\n",
    "  longest_path_node={}\n",
    "  for node in topsorting:\n",
    "    if node not in longest_path_node:\n",
    "      longest_path_node[node]=0\n",
    "    for neighbor in g[node]:\n",
    "      if neighbor not in longest_path_node:\n",
    "        longest_path_node[neighbor]=0\n",
    "      longest_path_node[neighbor]=max(longest_path_node[neighbor],longest_path_node[node]+1)\n",
    "  return max([longest_path_node[x] for x in longest_path_node]) if len(longest_path_node)!=0 else 0\n",
    "\n",
    "import itertools\n",
    "action_process_map={}\n",
    "def process_action(action):\n",
    "  if action in action_process_map:\n",
    "    return action_process_map[action]\n",
    "  action=action.lower()\n",
    "  action=re.sub(\"committees\",\"committee\",action)\n",
    "  committees=[committee for committee in committee_search_item if re.search(committee_search_item[committee].lower(),action) and not re.search(\"subcommittee\",action.lower())]\n",
    "  subcommittees=[committee for committee in committee_search_item if re.search(committee_search_item[committee].lower(),action) and re.search(\"subcommittee\",action.lower())]\n",
    "  if len(committees)>0:\n",
    "    committee_spellings_regex=\"|\".join([\"(\"+\")|(\".join(committee_spellings[committee])+\")\" for committee in committees]).lower()\n",
    "    action = re.sub(committee_spellings_regex,\"committee\",action)\n",
    "  if len(subcommittees)>0:\n",
    "    committee_spellings_regex=\"|\".join([\"(\"+\")|(\".join(committee_spellings[committee])+\")\" for committee in subcommittees]).lower()\n",
    "    action = re.sub(committee_spellings_regex,\"subcommittee\",action)\n",
    "  action=re.sub(r\"\\,|\\.|\\-\",\" \",action)\n",
    "  action=re.sub(r\"[0-9]\",\"\",action)\n",
    "  action=re.sub(r\" +\",\" \",action)\n",
    "  action = re.sub(r'\\b(mr|mrs|ms|senator|representative) \\w+\\s', 'representative ',action)\n",
    "  action = re.sub(r'(\\w+ hour)|(\\w+ minutes)', 'time',action)\n",
    "  action = re.sub(r'\\(.*?\\)', '',action)\n",
    "  action=re.sub(r\" +\",\" \",action)\n",
    "  action_process_map[action]=action\n",
    "  return action\n",
    "# def rename(action):\n",
    "#   # if action in action_process_map:\n",
    "#   #   return action_process_map[action]\n",
    "#   # action=action.lower()\n",
    "#   # action=re.sub(\"committees\",\"committee\",action)\n",
    "#   # committees=[committee for committee in committee_search_item if re.search(committee_search_item[committee].lower(),action) and not re.search(\"subcommittee\",action.lower())]\n",
    "#   # subcommittees=[committee for committee in committee_search_item if re.search(committee_search_item[committee].lower(),action) and re.search(\"subcommittee\",action.lower())]\n",
    "#   # if len(committees)>0:\n",
    "#   #   committee_spellings_regex=\"|\".join([\"(\"+\")|(\".join(committee_spellings[committee])+\")\" for committee in committees]).lower()\n",
    "#   #   action = re.sub(committee_spellings_regex,\"the committee\",action)\n",
    "#   # if len(subcommittees)>0:\n",
    "#   #   committee_spellings_regex=\"|\".join([\"(\"+\")|(\".join(committee_spellings[committee])+\")\" for committee in subcommittees]).lower()\n",
    "#   #   action = re.sub(committee_spellings_regex,\"the subcommittee\",action)\n",
    "#   # action=re.sub()\n",
    "#   # action=re.sub(r\"\\,|\\.|\\-\",\" \",action)\n",
    "#   # action=re.sub(r\"[0-9]\",\"\",action)\n",
    "#   # action=re.sub(r\" +\",\" \",action)\n",
    "#   # action = re.sub(r'\\b(mr|mrs|ms|senator|representative) \\w+\\s', 'representative ',action)\n",
    "#   # action = re.sub(r'(\\w+ hour)|(\\w+ minutes)', 'time',action)\n",
    "#   action = re.sub(r'\\(.*?\\)', '',action)\n",
    "#   # action=re.sub(r\" +\",\" \",action)\n",
    "#   # action_process_map[action]=action\n",
    "#   return action\n",
    "action_process_split_map={}\n",
    "def process_split_action(action):\n",
    "  if action in action_process_split_map:\n",
    "    return action_process_split_map[action]\n",
    "  processed_action=process_action(action)\n",
    "  processed_split_action=processed_action.split(\" \")\n",
    "  processed_split_action=[x for x in processed_split_action if x!=\"\"]\n",
    "  action_process_split_map[action]=processed_split_action\n",
    "  return processed_split_action\n",
    "\n",
    "def longest_common_subsequence_old(sentence1,sentence2):\n",
    "  if len(sentence1)==0 or len(sentence2)==0:\n",
    "    return 0\n",
    "  matches=[]\n",
    "  for i1,word1 in enumerate(sentence1):\n",
    "    for i2,word2 in enumerate(sentence2):\n",
    "      if word1==word2:\n",
    "        matches.append((i1,i2))\n",
    "  if len(matches)==0:\n",
    "    return 0\n",
    "  g = {}\n",
    "  for match1 in matches:\n",
    "    if match1 not in g:\n",
    "      g[match1]=[]\n",
    "    for match2 in matches:\n",
    "      if match2 not in g:\n",
    "        g[match2]=[]\n",
    "      if match1[0]<match2[0] and match1[1]<match2[1]:\n",
    "        g[match1].append(match2)\n",
    "  return longest_path(g)+1\n",
    "def edit_distance(l1,l2):\n",
    "  dp=np.empty((len(l2)+1,len(l1)+1))\n",
    "  dp[0]=range(len(l1)+1)\n",
    "  dp[:,0]=range(len(l2)+1)\n",
    "  for i in range(1,len(l2)+1):\n",
    "    for j in range(1,len(l1)+1):\n",
    "      dp[i,j]=min(dp[i][j-1]+1,dp[i-1][j]+1,dp[i-1][j-1] if l1[j-1]==l2[i-1] else dp[i-1][j-1]+1)\n",
    "  return dp[-1,-1]\n",
    "def longest_common_subsequence(l1, l2):\n",
    "  # Helper function to calculate edit distance without substitutions allowed\n",
    "  def edit_distance_no_subs(l1, l2):\n",
    "    # Create dp matrix with size (len(l2)+1) x (len(l1)+1)\n",
    "    dp = np.empty((len(l2)+1, len(l1)+1))\n",
    "    # Initialize first row and column\n",
    "    dp[0] = range(len(l1)+1)\n",
    "    dp[:,0] = range(len(l2)+1)\n",
    "    # Set infinity value for substitutions\n",
    "    inf = float('inf')  # Use float('inf') instead of len(l1)*len(l2)\n",
    "    \n",
    "    # Fill dp matrix\n",
    "    for i in range(1, len(l2)+1):\n",
    "      for j in range(1, len(l1)+1):\n",
    "        # If characters match, take diagonal value\n",
    "        # If not, take minimum of insert/delete plus 1\n",
    "        # Substitutions are not allowed (inf cost)\n",
    "        dp[i,j] = min(\n",
    "          dp[i][j-1] + 1,  # deletion\n",
    "          dp[i-1][j] + 1,  # insertion\n",
    "          dp[i-1][j-1] if l1[j-1] == l2[i-1] else inf  # match or substitution\n",
    "        )\n",
    "    return dp[-1][-1]\n",
    "\n",
    "  # Handle empty sequences\n",
    "  if not l1 or not l2:\n",
    "    return 0\n",
    "    \n",
    "  # LCS length = (len(l1) + len(l2) - edit_distance_no_subs) / 2\n",
    "  # Since each non-matching character requires one insertion and one deletion\n",
    "  distance = edit_distance_no_subs(l1, l2)\n",
    "  return (len(l1) + len(l2) - distance) // 2\n",
    "\n",
    "def test_edit_distance():\n",
    "  # Test empty strings\n",
    "  assert edit_distance([], []) == 0\n",
    "  # Test one empty string\n",
    "  assert edit_distance([], ['a']) == 1\n",
    "  assert edit_distance(['a'], []) == 1\n",
    "  # Test single character difference\n",
    "  assert edit_distance(['a'], ['b']) == 1\n",
    "  # Test same strings\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'b', 'c']) == 0\n",
    "  # Test insertion\n",
    "  assert edit_distance(['a', 'c'], ['a', 'b', 'c']) == 1\n",
    "  # Test deletion\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'c']) == 1\n",
    "  # Test substitution\n",
    "  assert edit_distance(['a', 'b', 'c'], ['a', 'd', 'c']) == 1\n",
    "  # Test multiple operations\n",
    "  assert edit_distance(['a', 'b'], ['c', 'd']) == 2\n",
    "\n",
    "# test_edit_distance()\n",
    "mustmatch_regexes=[\"subcommittee\"]\n",
    "def similar_words(action1,action2,threshold):\n",
    "  for regex in mustmatch_regexes:\n",
    "    if not (bool(re.search(regex,action1.lower() if regex.islower() else action1))==bool(re.search(regex,action2.lower() if regex.islower() else action2))):\n",
    "      return False\n",
    "  action1=process_split_action(action1)\n",
    "  action2=process_split_action(action2)\n",
    "  length=max(len(action1),len(action2))\n",
    "  return abs(len(action1)-len(action2))<threshold*length and edit_distance_below(action1,action2,threshold*length)\n",
    "def similar_string(action1,action2,threshold):\n",
    "  action1=process_action(action1)\n",
    "  action2=process_action(action2)\n",
    "\n",
    "  for regex in mustmatch_regexes:\n",
    "    if not (bool(re.search(regex,action1))==bool(re.search(regex,action2))):\n",
    "      return False\n",
    "  length=max(len(action1),len(action2))\n",
    "  return abs(len(action1)-len(action2))<threshold*length and edit_distance_below(action1,action2,threshold*length)\n",
    "\n",
    "\n",
    "# Test cases\n",
    "# print(\"Test 1:\", subsequence_similarity(\"hi my name is Bob\", \"hi Bob\") == 2)  # Should match \"hi\" and \"Bob\"\n",
    "# print(\"Test 2:\", subsequence_similarity(\"a b c\", \"a c b\") == 2)  # Should match \"a\" and either \"b\" or \"c\"\n",
    "# print(\"Test 3:\", subsequence_similarity(\"x y z\", \"a b c\") == 0)  # No common subsequence except single words\n",
    "# print(\"Test 4:\", subsequence_similarity(\"\", \" \") == 0)  # Empty strings\n",
    "# print(\"Test 5:\", subsequence_similarity(\"hello world\", \"hello there world\") == 2)  # \"hello\" and \"world\"\n",
    "\n",
    "# Checks if the edit distance between l1 and l2 is below threshold*max(len(l1),len(l2))\n",
    "def prefix_sum(l):\n",
    "  s=0\n",
    "  out=[]\n",
    "  for e in l:\n",
    "    s+=e\n",
    "    out.append(s)\n",
    "  return out\n",
    "def postfix_sum(l):\n",
    "  s=0\n",
    "  out=[]\n",
    "  for e in reversed(l):\n",
    "    s+=e\n",
    "    out.append(s)\n",
    "  return list(reversed(out))\n",
    "def edit_distance_below(l1,l2,threshold):\n",
    "  if abs(len(l1)-len(l2))>threshold:\n",
    "    return False\n",
    "  dp=np.empty((len(l2)+1,len(l1)+1))\n",
    "  dp[0]=range(len(l1)+1)\n",
    "  dp[:,0]=range(len(l2)+1)\n",
    "  min_dist=1\n",
    "  max_num=max(len(l2),len(l1))+1\n",
    "  for i in range(2*len(l2)+2):\n",
    "    min_dist_last=min_dist\n",
    "    min_dist=max_num\n",
    "    for j in range(max(i-len(l1)-1,0),min(i+1,len(l2)+1)):\n",
    "      col=i-j\n",
    "      row=j\n",
    "      if row > len(l2) or col > len(l1):\n",
    "        continue\n",
    "      if row>0 and col>0:\n",
    "        dp[row,col]=min(dp[row][col-1]+1,dp[row-1][col]+1,dp[row-1][col-1] if l2[row-1]==l1[col-1] else dp[row-1][col-1]+1)\n",
    "      if dp[row,col]<min_dist:\n",
    "        min_dist=dp[row,col]\n",
    "    if min_dist<max_num and min(min_dist,min_dist_last)>threshold:\n",
    "      return False\n",
    "  return dp[-1,-1]<=threshold\n",
    "def test_edit_distance_below():\n",
    "  # Test cases for edit_distance_below function\n",
    "  # Test empty strings\n",
    "  assert edit_distance_below([], [], 0) == True, \"Failed on empty strings\"\n",
    "  # Test one empty string\n",
    "  assert edit_distance_below([], ['a'], 0.5) == False, \"Failed on one empty string\"\n",
    "  assert edit_distance_below(['a'], [], 0.5) == False, \"Failed on one empty string\"\n",
    "  # Test single character difference\n",
    "  assert edit_distance_below(['a'], ['b'], 1.0) == True, \"Failed on single character difference\"\n",
    "  assert edit_distance_below(['a'], ['b'], 0.5) == False, \"Failed on single character difference\"\n",
    "  # Test same strings\n",
    "  assert edit_distance_below(['a', 'b', 'c'], ['a', 'b', 'c'], 1.5) == True, \"Failed on same strings\"\n",
    "  # Test insertion\n",
    "  assert edit_distance_below(['a', 'c'], ['a', 'b', 'c'], 1.5) == True, \"Failed on insertion\"\n",
    "  assert edit_distance_below(['a', 'c'], ['a', 'b', 'c'], 0.9) == False, \"Failed on insertion\"\n",
    "  # Test deletion\n",
    "  assert edit_distance_below(['a', 'b', 'c'], ['a', 'c'], 1.5) == True, \"Failed on deletion\"\n",
    "  assert edit_distance_below(['a', 'b', 'c'], ['a', 'c'], 0.9) == False, \"Failed on deletion\"\n",
    "  # Test substitution\n",
    "  assert edit_distance_below(['a', 'b', 'c'], ['a', 'd', 'c'], 1.5) == True, \"Failed on substitution\"\n",
    "  assert edit_distance_below(['a', 'b', 'c'], ['a', 'd', 'c'], 0.9) == False, \"Failed on substitution\"\n",
    "  # Test multiple operations\n",
    "  assert edit_distance_below(['a', 'b'], ['c', 'd'], 2) == True, \"Failed on multiple operations\"\n",
    "  assert edit_distance_below(['a', 'b'], ['c', 'd'], 1) == False, \"Failed on multiple operations\"\n",
    "  # Test longer strings\n",
    "  assert edit_distance_below(['a', 'b', 'c', 'd'], ['a', 'x', 'c', 'y'], 2) == True, \"Failed on longer strings\"\n",
    "  assert edit_distance_below(['a', 'b', 'c', 'd'], ['a', 'x', 'c', 'y'], 1.3) == False, \"Failed on longer strings\"\n",
    "\n",
    "  # Run the test cases\n",
    "test_edit_distance_below()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to process all the actions ahead of time\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "actions = list(set(history_df[\"action\"]))\n",
    "for action in actions:\n",
    "  process_split_action(action,action_process_map) # build up memo \n",
    "json.dump(action_process_map,open(\"./outputs/07-23/action_process_map.json\",\"w\"),indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_buckets_f(action):\n",
    "  if re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) in the nature of a substitute offered by\",action.lower()):\n",
    "    return \"H.Amdt. in the nature of a substitute offered by\"\n",
    "  if re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) offered by\",action.lower()):\n",
    "    return \"H.Amdt. offered by\"\n",
    "  if re.search(r\"^s\\.amdt\\.[0-9]*? amendment sa [0-9]*? proposed by\",action.lower()):\n",
    "    return \"S.Amdt. offered by\"\n",
    "  if re.search(r\"the.*? house.*? proceeded.*? with.*? of debate.*? on.*? the.*? amendment\",action.lower()):\n",
    "    return \"DEBATE - The House proceeded with 10 minutes of debate on the Broun (GA) motion to recommit with instructions\"\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(special_buckets_f(\"Floor summary: DEBATE - The House proceeded with 10 minutes of debate on the Delaney motion to recommit with instructions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(process_action(\"H.AMDT.1 Amendment (A001) offered by Mr. Becerra. (consideration: CR H6; text: CR H6)Amendment nominated Democrat candidates for Officers of the House.\"))\n",
    "print(process_action(\"H.AMDT.100 Amendment (A011) offered by Mr. Minnick. (consideration: CR H5029; text: CR H5029)Amendment provides that the amount of a balance following a notice of a rate increase would be protected from the rate of increase as of the 7-day mark instead of the 14-day mark.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "buckets_saved={}\n",
    "def bucket_actions(actions,similar,special_bucket_f = lambda x:None,bucket_names_f=lambda x:x):\n",
    "  special_buckets=defaultdict(list)\n",
    "  bucket_list=[]\n",
    "  \n",
    "  for i,action in enumerate(actions):\n",
    "    bucketed=False\n",
    "    if action_special_bucket:=special_bucket_f(action):\n",
    "      special_buckets[action_special_bucket].append(action)\n",
    "      bucketed=True\n",
    "    else:\n",
    "      for bucket in reversed(bucket_list):\n",
    "        if similar(bucket[0],action):\n",
    "          bucket.append(action)\n",
    "          bucketed=True\n",
    "          break\n",
    "    if i%1000==0:\n",
    "      print(i)\n",
    "    if not bucketed:\n",
    "      print(action)\n",
    "      bucket_list.append([action])\n",
    "  buckets = {bucket_names_f(bucket[0]):bucket for bucket in bucket_list}\n",
    "  buckets.update(special_buckets)\n",
    "  buckets_saved.update(buckets)\n",
    "  # bucket_list_saved.append(bucket_list)\n",
    "  return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket actions\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# actions=\n",
    "# actions = [sentence_split(action) for action in history_df[\"action\"]]\n",
    "\n",
    "# buckets=bucket_actions(lambda action1,action2:similar_string(action1,action2,1/7  ),special_bucket_f=special_buckets_f,bucket_names_f=lambda x:process_action(x))\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "actions = sorted(list(set(history_df[\"action\"])))\n",
    "buckets=bucket_actions(actions,lambda action1,action2:similar_string(action1,action2,1/7),special_bucket_f=special_buckets_f)\n",
    "bucket_names=list(buckets.keys())\n",
    "# buckets=bucket_actions(lambda action1,action2:similar_words(action1,action2,1/4),special_bucket_f=special_buckets_f)\n",
    "  \n",
    "\n",
    "# buckets_raw=[]\n",
    "# special_buckets_raw=[[],[],[]]\n",
    "# actions = \n",
    "# for i,action in enumerate(actions):\n",
    "#   bucketed=False\n",
    "#   # if re.search(r\"^debate - the house proceeded with 10 minutes of debate on the .*?motion to recommit\",action[1].lower()):\n",
    "#   #   special_buckets_raw[3].append(action)\n",
    "#   #   continue\n",
    "#   for bucket in reversed(buckets_raw):\n",
    "#     # if action[0]==bucket[0][0]:\n",
    "#     #   bucket.append(action)\n",
    "#     #   bucketed=True\n",
    "#     #   break\n",
    "#     if abs(len(action[0])-len(bucket[0][0]))<1/3*max(len(bucket[0][0]),len(action[0])) and subsequence_similarity(action[0],bucket[0][0])>=max(len(action[0])*2/3, len(bucket[0][0])*2/3):\n",
    "#       # print(action)\n",
    "#       bucket.append(action)\n",
    "#       bucketed=True\n",
    "#       break\n",
    "# buckets_raw.extend(special_buckets_raw)\n",
    "# buckets = [[action[1] for action in bucket if type(action[1])==str] for bucket in buckets_raw]\n",
    "# display(buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/buckets_after_refinement_08-01.json\",\"r\") as file:\n",
    "  buckets_refined=json.load(file)\n",
    "bucket_names_refined=sorted(list(buckets_refined.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buckets_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/buckets_after_refinement_08-01.json\",\"r\") as file:\n",
    "  buckets_refined=json.load(file)\n",
    "bucket_names_refined=sorted(list(buckets_refined.keys()))\n",
    "bucket_names_refined_combined = bucket_actions(bucket_names_refined,lambda name1, name2: edit_distance_below(name1,name2,1/7*max(len(name1),len(name2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_buckets(actions,f):\n",
    "  buckets=defaultdict(list)\n",
    "  for action in actions:\n",
    "    for bucket in f(action):\n",
    "      buckets[bucket].append(action)\n",
    "  return dict(buckets)\n",
    "starting_regexes=[r\"^All points of consideration against consideration.*? are waived\",\\\n",
    "         r\"^ANNOUNCEMENT\",r\"^APPOINTMENT OF CONFEREE\",r\"^APPOINTMENT OF.*? CONFEREE\",\\\n",
    "          r\"^forwarded by subcommittee to full committee\",r\"^GENERAL DEBATE\",r\"^Introduced in the Senate\",\\\n",
    "          r\"^MOMENT OF SILENCE\",r\"^Motion by \\[SENATOR\\] to (commit|refer) to\",r\"^Motion (by \\[SENATOR\\] )?to concur\",\\\n",
    "          r\"^Motion by \\[SENATOR\\] to instruct\",r\"^Motion by \\[SENATOR\\] to reconsider\",r\"^Motion to disagree\",\\\n",
    "          r\"^Motion to discharge\",r\"^Motion to proceed to consideration\",r\"^Motion to table the motion\",\n",
    "          r\"^Motion to waive\",r\"^NOTIFICATION OF INTENT\",\"^ORDER OF BUSINESS\",\"^ORDER OF PROCEDURE\",\n",
    "          r\"^On agreeing to the resolution\",r\"On agreeing to the \\[REPRESENTATIVE\\] amendment\",\\\n",
    "          r\"On motion that the House (dis)?agree to the Senate amendment\",\\\n",
    "          r\"On motion that the House agree with an amendment\",\\\n",
    "          r\"On motion that the House (dis)?agree\",\n",
    "          r\"On motion that the House instruct conferees\",\n",
    "          r\"^On motion (that the House |to )suspend the rules\",r\"^On passage\",r\"^On ordering the previous question\",\n",
    "          r\"^On questioning of consideration\",r\"^Ordered to be [r|R]eported\",r\"^point of order\",\n",
    "          r\"^POSTPONED CONSIDERATION OF VETO MESSAGE\",r\"^POSTPONED PROCEEDING\",r\"POSTPONED ROLL CALL VOTE\",\n",
    "          r\"^Passed Senate\",r\"^Previous question shall be considered as ordered\",\\\n",
    "          r\"Previous question shall be considered as ordered\",\\\n",
    "          r\"^Provides for consideration of \\[BILL\",r\"^Provides for consideration of the Senate Amendment\",\\\n",
    "          r\"^Provides for consideration\", r\"^Provides for( \\w+)? \\[TIME\\] of.*? debate\",\\\n",
    "          r\"^Pursuant to \\[RESOLUTION\\]\", r\"^Pursuant to a previous( \\w+)? order\", r\"Pursuant to clause\",\\\n",
    "          r\"^Pursuant to the order of\",\\\n",
    "          r\"^proceeded with.*? debate\",r\"^Pursuant to the provisions of \\[RESOLUTION\\]\",r\"^QUESTION OF CONSIDERATION\",\\\n",
    "          r\"^QUESTION OF CONSIDERATION\",r\"^QUESTION OF THE PRIVILEGES OF THE HOUSE\",r\"^Received in the Senate\",\\\n",
    "          r\"^received in the senate[\\,\\.] read\",\\\n",
    "          r\"^Referred\",r\"^Resolution agreed to in Senate\",r\"Resolution provides for( \\w+)? \\[TIME\\] of general debate\",\\\n",
    "          r\"(Rule|Resolution) provides for( \\w+)? \\[TIME\\] of general debate\",\"debate\",\\\n",
    "          r\"^Resolution provides for consideration of( \\w+)? \\[BILL\",r\"^Resolving differences\",r\"^Rule provides for consideration\",\\\n",
    "          r\"^Rules Committee Resolution\",r\"^Ruling of the Chair\",r\"^Second cloture\", r\"See( \\w+)\\[BILL\\] \",\n",
    "          r\"^Senate Committee.*? discharged\",r\"^Senate agreed to.*? amendment\",r\"^Senate agreed to conference report\",\\\n",
    "            r\"^Senate agreed to\",r\"^Senate appointed conferee\",r\"Senate concurred in\", r\"Senate disagree. to.*? House amendment\",\n",
    "            r\"^Senate insists on its amendment\",r\"^Senate passed companion measure\", r\"^Senate returned papers\", r\"^Senate vitiated previous\",\n",
    "            r\"^Star Print ordered\",r\"^The Chair announced\",r\"^The Chair\",\"postponed\",r\"^The Committee of the Whole\",\n",
    "            \"The Committee of the Whole proceeded\",\n",
    "            r\"^The Committee rose informally\",r\"The House proceeded with.*? \\[TIME\\] of debate\",r\"^The House resolved into Committee of the Whole\",\n",
    "            r\"^The Speaker appointed (additional )?conferees\"]\n",
    "other_regexes=[r\"^On motion\",\"suspend the rules\"]\n",
    "def special_buckets_f(action):\n",
    "  out=[]\n",
    "  for regex in regexes:\n",
    "    if re.search(regex,action.lower() if regex.islower() else action):\n",
    "      out.append(action)\n",
    "  return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bucket_names_refined))\n",
    "print(len(bucket_names_refined_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_refined_combined={}\n",
    "unbucketed=[]\n",
    "for combination in bucket_names_refined_combined:\n",
    "  buckets_refined_combined[combination]=[]\n",
    "  for name in bucket_names_refined_combined[combination]:\n",
    "    if name not in buckets_refined_combined:\n",
    "      unbucketed.append(name)\n",
    "      continue\n",
    "    buckets_refined_combined[combination].extend(buckets_refined[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/buckets_refined_combined_08-04.json\",\"w\") as file:\n",
    "  json.dump(buckets_refined_combined,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buckets_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buckets_refined_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(bucket_list_saved)\n",
    "\n",
    "# special_buckets={buckets:bucket for name in buckets if }\n",
    "buckets = {process_action(bucket[0]):bucket for bucket in bucket_list_saved[0]}\n",
    "buckets.update(special_buckets)\n",
    "# bucket_list_names=[process_action(bucket[0])]\n",
    "\n",
    "# buckets.update(special_buckets)\n",
    "display(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save buckets\n",
    "import json\n",
    "from datetime import datetime\n",
    "with open(f\"./outputs/buckets_{datetime.now().strftime(f\"%m-%d\")}_charsim17.json\",\"w\") as file:\n",
    "  json.dump(buckets,file,indent=2)\n",
    "with open(f\"./outputs/bucket_names_{datetime.now().strftime(f\"%m-%d\")}_charsim17.txt\",\"w\") as file:\n",
    "  for name in bucket_names:\n",
    "    file.write(name)\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets=special_buckets\n",
    "display(len(buckets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"outputs/buckets_07-29_textsimilarity17.json\",\"r\") as file:\n",
    "  buckets = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_buckets(fs,buckets):\n",
    "  out=defaultdict(list)\n",
    "  for bucket_name in buckets:\n",
    "    moved=False\n",
    "    for f in fs:\n",
    "      if f_bucket:= f(bucket_name):\n",
    "        out[f_bucket].extend(bucket)\n",
    "        moved=True\n",
    "        break\n",
    "    if not moved:\n",
    "      out[bucket_name].append(action)\n",
    "  out=dict(out)\n",
    "  return out\n",
    "def make_new_buckets(fs,buckets):\n",
    "  out=defaultdict(list)\n",
    "  for bucket_name in buckets:\n",
    "    for action in buckets[bucket_name]:\n",
    "      # print(bucket)\n",
    "      moved=False\n",
    "      for f in fs:\n",
    "        if f_bucket:= f(action):\n",
    "          out[f_bucket].append(action)\n",
    "          moved=True\n",
    "          break\n",
    "      if not moved:\n",
    "        out[bucket_name].append(action)\n",
    "  out=dict(out)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) in the nature of a substitute offered by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n",
    "\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^h\\.amdt\\.[0-9]*? amendment \\(.*?\\) offered by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n",
    "# buckets_manual=combine_buckets(lambda bucket:re.search(r\"^s\\.amdt\\.[0-9]*? amendment sa [0-9]*? proposed by\",bucket[0].lower()) if type(bucket)==list and len(bucket)>0 else False,buckets_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_action_to_bucket_name={}\n",
    "for bucket in buckets:\n",
    "  for action in bucket:\n",
    "    specific_action_to_bucket_name[action]=bucket[0]\n",
    "# display(specific_action_to_bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket_map={action:bucket for bucket in buckets for action in buckets_saved[bucket]}\n",
    "bucket_map={}\n",
    "for name in buckets:\n",
    "  for action in buckets[name]:\n",
    "    bucket_map[action]=name\n",
    "# display(bucket_map)\n",
    "bucket_sizes={name:0 for name in buckets}\n",
    "unbucketed=[]\n",
    "bucketed=[]\n",
    "for action in history_df[\"action\"]:\n",
    "  if action not in bucket_map:\n",
    "    unbucketed.append(action)\n",
    "    continue\n",
    "  bucketed.append(action)\n",
    "  bucket_sizes[bucket_map[action]]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets.update(new_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorted(set(unbucketed)))\n",
    "display(len(unbucketed))\n",
    "display(len(bucketed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "def nums_to_bars(nums):\n",
    "  counter = Counter(nums)\n",
    "  keyvals=counter.items()\n",
    "  print(sorted(keyvals))\n",
    "\n",
    "  return [key for key,val in keyvals],[val for key,val in keyvals]\n",
    "xs,heights = nums_to_bars(bucket_sizes.values())\n",
    "ax.bar(xs,heights)\n",
    "ax.loglog()\n",
    "# ax.set_ybound(0,100)\n",
    "# ax.set_xbound(0,100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_names = [bucket[0] for bucket in buckets if type(bucket)==list and len(bucket)>0]\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "history_df[\"bucket\"]=history_df[\"action\"].apply(lambda action:specific_action_to_bucket_name[action])\n",
    "llm_buckets = []\n",
    "with open(\"./outputs/bucket_names_output.jsonl\", \"r\") as file:\n",
    "  for line in file:\n",
    "    if line.strip():  # Skip empty lines\n",
    "      response = json.loads(line)\n",
    "      llm_buckets.append(response[\"response\"][\"body\"][\"output\"][0][\"content\"][0][\"text\"])\n",
    "      \n",
    "history_df[\"llm_bucket\"] = history_df[\"bucket\"].map(dict(zip(bucket_names, llm_buckets)))\n",
    "\n",
    "# Add next bill_id column to compare\n",
    "history_df[\"next_bill_id\"]=history_df[\"bill_id\"].shift(-1)\n",
    "history_df[\"next_bucket\"]=history_df[\"bucket\"].shift(-1)\n",
    "history_df[\"next_bucket\"]=history_df.apply(lambda row:row[\"next_bucket\"] if row[\"next_bill_id\"]==row[\"bill_id\"] else None,1)\n",
    "history_df[\"next_bucket_llm\"]=history_df[\"llm_bucket\"].shift(-1)\n",
    "history_df[\"next_llm_bucket\"]=history_df.apply(lambda row:row[\"next_bucket_llm\"] if row[\"next_bill_id\"]==row[\"bill_id\"] else None,1)\n",
    "\n",
    "g={}\n",
    "for bucket1 in llm_buckets:\n",
    "  g[bucket1]={}\n",
    "  for bucket2 in llm_buckets:\n",
    "    g[bucket1][bucket2]=0\n",
    "  g[bucket1][None]=0\n",
    "for i, row in history_df.iterrows():\n",
    "  g[row[\"llm_bucket\"]][row[\"next_llm_bucket\"]]+=1\n",
    "# display(g)\n",
    "\n",
    "\n",
    "\n",
    "# display(history_df)\n",
    "# for i,row in history_df.iterrows():\n",
    "#   print(row.bucket)\n",
    "# for action in history_df:\n",
    "#   history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket1 in llm_buckets:\n",
    "  for bucket2 in llm_buckets:\n",
    "    if g[bucket1][bucket2]>200:\n",
    "      print(bucket1+\" -> \"+bucket2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch to come up with names for each bucket\n",
    "import random\n",
    "import numpy as np\n",
    "bucket_names_to_bucket={bucket[0]:bucket for bucket in buckets if type(bucket)==list and len(bucket)>0}\n",
    "manual_name_to_llm_reponse={}\n",
    "random.seed(1423)\n",
    "bucket_names_str=\"\\n\".join(random.sample(bucket_names,k=20))\n",
    "BATCHES=4\n",
    "bucket_names_batches=np.split(np.array(bucket_names),BATCHES)\n",
    "for batch_i in range(BATCHES):\n",
    "  input_len=0\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_input.jsonl\",\"w\") as file:\n",
    "    for i,bucket_name in enumerate(bucket_names_batches[batch_i]):\n",
    "      input=f\"\"\"I am analyzing a dataset full of Congressional actions. I have categorized these actions into buckets,\\\n",
    "      where each bucket has near-identical actions, usually differing only in the names/numbers of congresspeople,bills,votes,and committees. \\\n",
    "      I want to come up with names for these buckets. These names are usually just the actions with names/numbers of congresspeople, bills, votes and committees removed.\n",
    "      First, here is a sample of one action from the 20 buckets chosen at random:\\n\n",
    "      {bucket_names_str}\\n\n",
    "      Here are some example names you might give to bucket actions:\\n\n",
    "      Action from bucket: Mr. Whitfield moved that the Committee now rise.\\n\n",
    "      Bucket name: Congressperson moved the Committee now rise.\\n\\n\n",
    "      Action from bucket: The House resumed with the motion to agree in the Senate amendment to H.R. 83, with an amendment. (consideration: CR H9284-9290)\\n\n",
    "      Bucket name: The House resumed with the motion to agree in the Senate amendment to the bill, with an amendment.\\n\\n\n",
    "      Action from bucket: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee on Capital Markets and Government Sponsored Enterprises Prior to Introduction and Referral Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee on Capital Markets and Government Sponsored Enterprises\\n\n",
    "      Bucket name: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee Prior to Introduction and Referral. Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee\\n\\n\n",
    "      Action from bucket: Passed Senate with amendments by Yea-Nay. 93 - 7. Record Vote Number: 218.\\n\n",
    "      Bucket name: Passed Senate with amendments by Yea-Nay.\\n\\n\n",
    "      \n",
    "      Now, please come up with a name for the bucket with the following actions. Please only give me the name, nothing else in your response.\n",
    "      If there is only one line, it means the bucket has only one action:\\n\n",
    "      {\"\\n\".join(random.sample(bucket_names_to_bucket[bucket_name],k=20) if len(bucket_names_to_bucket[bucket_name])>20 else bucket_names_to_bucket[bucket_name])}\"\"\"\n",
    "    \n",
    "      input_len+=len(input)\n",
    "      json.dump({\"custom_id\":\"action\"+str(i),\"url\":\"/v1/responses\",\"method\":\"POST\",\"body\":{\"input\":input,\"model\":\"gpt-4.1-mini\"}}, file)\n",
    "      file.write(\"\\n\")\n",
    "    # manual_name_to_llm_reponse[bucket_name]=client.responses.create(input=input,model=\"gpt-4.1-mini\")\n",
    "    # print(bucket_name)\n",
    "  print(input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "import asyncio\n",
    "import json\n",
    "dotenv.load_dotenv()\n",
    "client = openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "for bucket_name in bucket_names[:5]:\n",
    "      input=f\"\"\"I am analyzing a dataset full of Congressional actions. I have categorized these actions into buckets,\\\n",
    "      where each bucket has near-identical actions, usually differing only in the names/numbers of congresspeople,bills,votes,and committees. \\\n",
    "      I want to come up with names for these buckets. These names are usually just the actions with names/numbers of congresspeople, bills, votes and committees removed.\n",
    "      First, here is a sample of one action from the 20 buckets chosen at random:\\n\n",
    "      {bucket_names_str}\\n\n",
    "      Here are some example names you might give to bucket actions:\\n\n",
    "      Action from bucket: Mr. Whitfield moved that the Committee now rise.\\n\n",
    "      Bucket name: Congressperson moved the Committee now rise.\\n\\n\n",
    "      Action from bucket: The House resumed with the motion to agree in the Senate amendment to H.R. 83, with an amendment. (consideration: CR H9284-9290)\\n\n",
    "      Bucket name: The House resumed with the motion to agree in the Senate amendment to the bill, with an amendment.\\n\\n\n",
    "      Action from bucket: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee on Capital Markets and Government Sponsored Enterprises Prior to Introduction and Referral Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee on Capital Markets and Government Sponsored Enterprises\\n\n",
    "      Bucket name: Subcommittee Consideration and Mark-up Session Held and Forwarded to Full Committee by the Subcommittee Prior to Introduction and Referral. Revised discussion draft, as amended, ordered favorably reported to the full committee by the subcommittee\\n\\n\n",
    "      Action from bucket: Passed Senate with amendments by Yea-Nay. 93 - 7. Record Vote Number: 218.\\n\n",
    "      Bucket name: Passed Senate with amendments by Yea-Nay.\\n\\n\n",
    "\n",
    "      Now, please come up with a name for the bucket with the following actions. Please only give me the name, nothing else in your response.\n",
    "      If there is only one line, it means the bucket has only one action:\\n\n",
    "      {\"\\n\".join(random.sample(bucket_names_to_bucket[bucket_name],k=20) if len(bucket_names_to_bucket[bucket_name])>20 else bucket_names_to_bucket[bucket_name])}\"\"\"\n",
    "      response=client.responses.create(input=input,model=\"gpt-4.1-mini\")\n",
    "      print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI client\n",
    "import dotenv\n",
    "import os\n",
    "import openai\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "client = openai.Client(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "statuses=[batch.status for batch in client.batches.list()]\n",
    "while \"cancelling\" in statuses or \"in_progress\" in statuses:\n",
    "  time.sleep(60)\n",
    "  statuses=[batch.status for batch in client.batches.list()]\n",
    "print(\"starting batches\")\n",
    "for batch_i in range(3,4):\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_input.jsonl\",\"rb\") as file:\n",
    "    batch_file=client.files.create(file=file,purpose=\"batch\")\n",
    "  batch=client.batches.create(input_file_id=batch_file.id,endpoint=\"/v1/responses\",completion_window=\"24h\")\n",
    "  while batch.status!=\"completed\":\n",
    "    if batch.status==\"failed\":\n",
    "      break\n",
    "    time.sleep(60)\n",
    "  with open(f\"./outputs/bucket_names_batch{batch_i}_output.jsonl\",\"w\") as file:\n",
    "    file.write(client.files.content(file_id=client.batches.retrieve(batch_id=batch.id).output_file_id).text)\n",
    "  print(f\"finished batch {batch_i}\")\n",
    "  time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./outputs/bucket_names_batch3_output.jsonl\",\"w\") as file:\n",
    "  file.write(client.files.content(file_id=client.batches.retrieve(batch_id=\"batch_687e820d9b448190be9761b57efa014d\").output_file_id).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./outputs/bucket_names_output.jsonl\", \"w\") as outfile:\n",
    "  for batch_i in range(4):\n",
    "    with open(f\"./outputs/bucket_names_batch{batch_i}_output.jsonl\", \"r\") as infile:\n",
    "      outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.cancel(batch_id=batch.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.retrieve(batch_id=batch.id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./outputs/buckets_manual3.json\",\"w\") as file:\n",
    "  json.dump(buckets_manual,file)\n",
    "  # for bucket in buckets:\n",
    "  #   file.write(str(bucket))\n",
    "  # file.writelines(buckets_manual)\n",
    "# display([bucket for bucket in sorted(buckets_manual, key=lambda x:len(x),reverse=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "bucket_name_to_bucket={bucket[0]:bucket for bucket in buckets_manual if type(bucket)==list and len(bucket)>0}\n",
    "for bucket in bucket_name_to_bucket:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(set(history_df[\"action\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_buckets=[[] for bucket in buckets]\n",
    "misclassified_by_subsequence=[]\n",
    "responses=[]\n",
    "for i in range(13,len(buckets)):\n",
    "  bruteforce_bucket=buckets[i]\n",
    "  print(i,bruteforce_bucket[0])\n",
    "  responses=[]\n",
    "  async with asyncio.TaskGroup() as tg:\n",
    "    for action in bruteforce_bucket:\n",
    "      input = \"These are two actions taken from a dataset of Congressional Actions. Please tell me if these two actions are the same except for potentially different names/numbers of committees, senators, representatives, bills, motions, or amendments, and potentially different vote records as well. Give me a one word answer \\\"Yes\\\" or \\\"No\\\":\\n\"\\\n",
    "        + action+\"\\n\"+bruteforce_bucket[0]\n",
    "      print(action)\n",
    "      responses.append((action,tg.create_task(client.responses.create(input=input,model=\"gpt-4.1-nano\"))))\n",
    "  for response in responses:\n",
    "    if response[1].result().output_text == \"Yes\":\n",
    "      llm_buckets[i].append(response[0])\n",
    "    else:\n",
    "      misclassified_by_subsequence.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df=pd.concat([pd.read_csv(dataset) for dataset in datasets])\n",
    "\n",
    "# bills_df_updated = bills_df.copy(   )\n",
    "actions=set()\n",
    "referred=set()\n",
    "ldict = {\"committee\":{},\"general\":[],\"Submitted in the Senate\":[],\"Introduced in House\":[]}\n",
    "subcommittee_dict={\"misc\":[]}\n",
    "committee_cats=[]\n",
    "\n",
    "\n",
    "committee_dict=defaultdict(list)\n",
    "# for regex in committee_cats:\n",
    "#   subcommittee_dict[regex]=[]\n",
    "committee_regexes = [\n",
    "  r'house rose',\n",
    "  r'committee rose',\n",
    "  r'moved that the committee rise',\n",
    "  r'committee rise agreed to by voice vote',\n",
    "  r'Reported \\(Amended\\) by',\n",
    "  r'submitted in the senate, considered, and agreed to',\n",
    "  r'motion by senator',\n",
    "  r'Committee of the Whole House on the state of the Union',\n",
    "  r'reported an original measure',\n",
    "  r'Ordered to be reported .*? favorably',\n",
    "  r'Original measure reported to Senate',\n",
    "  r'the speaker designated the honorable',\n",
    "  r'Granted an extension for further consideration',\n",
    "  r'motion to discharge committee filed',\n",
    "  r'withdrawn by unanimous consent',\n",
    "  r'asked unanimous consent',\n",
    "  r'motion to refer the bill',\n",
    "  r'the speaker appointed [conferees|additional conferees]',\n",
    "  r'filed written report',\n",
    "  r'motion to discharge senate committee',\n",
    "  r'the committee substitute as amended agreed to by unanimous consent',\n",
    "  r'the committee amendment as amended agreed to by unanimous consent',\n",
    "  r'supplemental report filed by',\n",
    "  r'Pursuant to clause 6\\(h\\) of rule XVIII, the Committee of the Whole resumed its sitting',\n",
    "  r'by direction of .*? and asked for its immediate consideration',\n",
    "  r'motion to discharge the committee on rules',\n",
    "  r'motion to table the motion',\n",
    "  r'reported adversely.*? by the committee',\n",
    "  r'rules committee resolution .*? reported to house',\n",
    "  r'on motion that the committee now rise agreed to .*?[by voice vote|without objection]',\n",
    "  r'failed to report favorably',\n",
    "  r'ordered to be reported favorably',\n",
    "  r'The resolution provides? that.*? amendment in the nature of a substitute.*? shall be considered as adopted',\n",
    "  r'committee amendments?.*? agreed to by unanimous consent',\n",
    "  r'moved to commit to the committee',\n",
    "  r'committee agreed to seek consideration under suspension of the rules',\n",
    "  r'all points of order against consideration .*? are waived',\n",
    "  \"committee consideration and mark-up session held\",\n",
    "  \"hearings held\",\n",
    "  r\"committee.*? discharged\",\n",
    "  r\"reported by.*?with.*?report\",\n",
    "  r\"reported by.*?without.*?report\",\n",
    "  \"forwarded by subcommittee to full committee.*? by voice vote\",\n",
    "  \"forwarded by subcommittee to full committee.*? by unanimous consent\",\n",
    "  \"forwarded by subcommittee to full committee.*? by the yeas and nays\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by the yeas and nays\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by voice vote\",\n",
    "  \"forwarded by subcommittee to full committee.*? in the nature of a substitute .*?by unanimous consent\",\n",
    "  r\"amendment.*? offered by\",\n",
    "  \"it shall be in order to consider as an original bill for the purpose of amendment\",\n",
    "  \"moved to recommit\",\n",
    "  \"moved that the committee now rise\",\n",
    "  r\"[read twice and referred to]|[read the second time and referred to]\",\n",
    "  r\"^reported by\",\n",
    "  r\"amdt.*?referred to\",\n",
    "  r\"amendment.*? proposed by\",\n",
    "  r\"the committee resumed? it\\'?s sitting\",\n",
    "  r\"by[| the] direction of the committee on rules,.*? called up\",\n",
    "  \"rule provides for consideration of\",\n",
    "  \"resolution provides for consideration of\",\n",
    "  r\"^referred\",\n",
    "  r\"^re-referred\",\n",
    "  \"^received in the senate and referred\"\n",
    "]\n",
    "for row in history_df.itertuples():\n",
    "  # print(row.action)\n",
    "  # match = re.match(r'^referred to the (house )?(committee|subcommittee)', row.action.lower())\n",
    "  categorized=False\n",
    "  if re.search(\"committee\",row.action.lower()) and not re.search(\"committee of the whole\",row.action.lower()):\n",
    "    for regex in committee_regexes:\n",
    "      if re.search(regex,row.action.lower() if regex.islower() else row.action):\n",
    "        committee_dict[regex].append(row.action)\n",
    "        categorized=True\n",
    "        break\n",
    "    if not categorized:\n",
    "      committee_dict[\"misc\"].append(row.action)\n",
    "  elif re.search(\"Submitted in the Senate\",action):\n",
    "    ldict[\"Submitted in the Senate\"].append(action)\n",
    "  elif re.match(r\"Introduced in House$\",action):\n",
    "    ldict[\"Introduced in House\"].append(action)\n",
    "  else:\n",
    "    ldict[\"general\"].append(action)\n",
    "# committee_dict=dict(committee_dict)\n",
    "display(sorted(committee_dict[\"misc\"]))\n",
    "# display(ldict[\"general\"])\n",
    "# display(ldict[\"committee\"])\n",
    "# for key in ldict:\n",
    "#   display(ldict[key])\n",
    "# display(ldict)\n",
    "# for key in types:\n",
    "#   print(key,types[key])\n",
    "\n",
    "\n",
    "# bills_df[\"passed\"] = bills_df[]\n",
    "# history_df['referred to committee'] = history_df['action'].str.lower().str.match(r'^referred to the (house committee|committee|house subcommittee|subcommittee)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not re.search(r\"general debate\",\"GENERAL DEBATE - The Committee of the Whole proceeded with one hour of general debate on H.R. 277.\".lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Reported (Amended) by\" in \"Reported (Amended) by the Committee on Natural Resources. H. Rept. 118-919.\":\n",
    "  print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(\"embedding_batch.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "print(batch_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.create(input_file_id=batch_input_file.id,endpoint=\"/v1/outputs\",completion_window=\"24h\",metadata={\"description\":\"nightly eval job\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in client.files.content(\"file-Nybj7TyzxjuL7NXTWsNqGw\").text.split(\"\\n\"):\n",
    "  print(json.loads(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./outputs/bucket_names_batch0_output.jsonl\",\"w\") as file:\n",
    "  file.write(client.files.content(\"file-Nybj7TyzxjuL7NXTWsNqGw\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list(client.files.retr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(client.batches.retrieve(\"batch_685462b110ac8190bb51093fa9c12910\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = client.files.content(\"file-Vt5fNxGnSo2qh796HC27PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_response = client.files.content(\"file-Vt5fNxGnSo2qh796HC27PC\")\n",
    "# with open(\"outputs/2025-26-history.jsonl\",\"w\") as f:\n",
    "#   f.write(file_response.text)\n",
    "json_file = pd.read_json(\"outputs/2025-26-history.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.cluster\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "\n",
    "print(json_file.iloc[0])\n",
    "df = pd.DataFrame()\n",
    "df[\"bill#\"]=json_file[\"custom_id\"]\n",
    "# df[\"bill#\"]=json_file.apply(lambda x:x[\"custom_id\"])\n",
    "df[\"embedding\"]=json_file[\"response\"].apply(lambda x: x[\"body\"][\"data\"][0][\"embedding\"])\n",
    "df.sort_values(by=\"bill#\")\n",
    "outputs_array = np.array(df[\"embedding\"].tolist())\n",
    "# tsne_result = sklearn.manifold.TSNE().fit_transform(embeddings_array)\n",
    "# display(df)\n",
    "# display\n",
    "# (json_file[\"response\"].iloc[0])\n",
    "# display(type(json_file[\"custom_id\"]))\n",
    "# json_file.iloc[0][\"response\"][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_df = pd.read_csv(\"data/US/2025-2026_119th_Congress/csv/history.csv\")\n",
    "def bucket(cluster_per_action):\n",
    "  clusters={}\n",
    "  for i,cluster in enumerate(cluster_per_action):\n",
    "    cluster=int(cluster)\n",
    "    if cluster not in clusters:\n",
    "      clusters[cluster]=set()\n",
    "    clusters[cluster].add(history_df.iloc[i][\"action\"])\n",
    "  return list(clusters.values())\n",
    "def unique_actions(buckets):\n",
    "  buckets_unique=[]\n",
    "  for bucket in buckets:\n",
    "    bucket_unique={re.sub(\"-\",\"\",re.sub(r'\\d+', '', x)) for x in bucket}\n",
    "    buckets_unique.append(list(bucket_unique))\n",
    "  return buckets_unique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optics_result = sklearn.cluster.OPTICS().fit_predict(embeddings_array)\n",
    "# optics_buckets = unique_actions(bucket(optics_result))\n",
    "# display(optics_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "topleft_pts=[i for i,x in enumerate(tsne_result) if x[0]<-90 and x[1]>0]\n",
    "input_file=[json.loads(x)[\"body\"][\"input\"] for x in open(\"embedding_batch.jsonl\", \"rb\")]\n",
    "\n",
    "def plot_2d(arr):\n",
    "  plt.scatter([x[0] for x in arr],[x[1] for x in arr])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "\n",
    "intro_house=[i for i in range(len(input_file)) if input_file[i]==\"Introduced in House\"]\n",
    "# np.mean()\n",
    "means_overall=np.mean(embeddings_array,axis=0)\n",
    "std_overall = np.std(embeddings_array,axis=0)\n",
    "means_house=np.mean(embeddings_array[intro_house],axis=0)\n",
    "std_house = np.std(embeddings_array[intro_house],axis=0)\n",
    "print(np.mean(std_overall))\n",
    "print(np.mean(std_house))\n",
    "# fig, axs = plt.subplots(2)\n",
    "# axs[0].bar(range(len(means_overall)),means_overall-means_house)\n",
    "# ax[1].bar(range(len(means_overall)),std_overall)\n",
    "# display(np.std(embeddings_array[intro_house]))\n",
    "# display([input_file[x] for x in topleft_pts])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO05QBEL1BiZJV8jZLZO8J/",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "billanalysis-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
