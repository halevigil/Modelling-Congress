{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"outputs/buckets_07-29_charsim17.json\",\"r\") as file:\n",
    "  buckets = json.load(file)\n",
    "bucket_map={}\n",
    "for name in buckets:\n",
    "  for action in buckets[name]:\n",
    "    bucket_map[action]=name\n",
    "common_bucket_names = [name for name in buckets.keys() if len(buckets[name])>=10]\n",
    "common_bucket_names_inv={name:i for i,name in enumerate(common_bucket_names)}\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df_by_term=[pd.read_csv(ds) for ds in datasets]\n",
    "for i,df in enumerate(history_df_by_term):\n",
    "  df[\"term\"]=i+111\n",
    "history_df=pd.concat(history_df_by_term)\n",
    "# with open(\"./outputs/bucket_names_output.jsonl\", \"r\") as file:\n",
    "#   for line in file:\n",
    "#     if line.strip():  # Skip empty lines\n",
    "#       response = json.loads(line)\n",
    "#       llm_buckets.append(response[\"response\"][\"body\"][\"output\"][0][\"content\"][0][\"text\"])\n",
    "      \n",
    "# history_df[\"llm_bucket\"] = history_df[\"bucket\"].map(dict(zip(bucket_names, llm_buckets)))\n",
    "\n",
    "# Add next bill_id column to compare\n",
    "history_df[\"bucket\"]=history_df[\"action\"].apply(lambda action:bucket_map[action])\n",
    "bills={bill_id:group for (bill_id,group) in history_df.groupby(\"bill_id\")}\n",
    "bill_ids = list(bills.keys())\n",
    "\n",
    "MIN_TERM=min(history_df[\"term\"])\n",
    "MAX_TERM=max(history_df[\"term\"])\n",
    "N_TERMS=MAX_TERM-MIN_TERM+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(buckets.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(common_bucket_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha=2/3\n",
    "data=[]\n",
    "\n",
    "bill_id=history_df.iloc[0][\"bill_id\"]\n",
    "predecessors_vec=np.zeros(len(common_bucket_names))\n",
    "predecessor_vec=np.zeros(len(common_bucket_names))\n",
    "one_hot_curr = np.zeros(len(common_bucket_names))\n",
    "inputs=[]\n",
    "outputs=[]\n",
    "n_predecessors_prev=0\n",
    "for i,row in history_df.iterrows():\n",
    "  one_hot_prev = np.array(one_hot_curr)\n",
    "  one_hot_curr = np.zeros(len(common_bucket_names))\n",
    "  output_vec = np.zeros(len(common_bucket_names)+1)\n",
    "  if row.bucket in common_bucket_names_inv:\n",
    "    one_hot_curr[common_bucket_names_inv[row.bucket]]=1\n",
    "    output_vec[common_bucket_names_inv[row.bucket]]=1\n",
    "  else:\n",
    "    output_vec[-1]=1\n",
    "  \n",
    "  chamber = np.zeros(2)\n",
    "  if row[\"chamber\"]==\"House\":\n",
    "    chamber[0]=1\n",
    "  if row[\"chamber\"]==\"Senate\":\n",
    "    chamber[1]=1\n",
    "\n",
    "  # if sum(one_hot_prev)!=1 and sum(one_hot_prev)!=0:\n",
    "  #   print(\"non-0/1 sum\",i)\n",
    "  # n_predecessors=np.sum(predecessors_vec)\n",
    "  # if n_predecessors!=0 and n_predecessors!=n_predecessors_prev+1 and n_predecessors!=n_predecessors_prev:\n",
    "  #   print(n_predecessors,n_predecessors_prev,i)\n",
    "  # n_predecessors_prev=n_predecessors\n",
    "  if row[\"bill_id\"]!=bill_id:\n",
    "    bill_id=row[\"bill_id\"]\n",
    "    one_hot_prev=np.zeros(len(common_bucket_names))\n",
    "    predecessors_vec=np.zeros(len(common_bucket_names))\n",
    "    predecessor_vec=np.zeros(len(common_bucket_names))\n",
    "  predecessor_vec=(1-alpha)*one_hot_prev+alpha*np.array(predecessor_vec)\n",
    "  predecessors_vec=np.array(predecessors_vec)+one_hot_prev\n",
    "  one_hot_term=np.zeros(N_TERMS)\n",
    "  one_hot_term[row.term-MIN_TERM]=1\n",
    "  # input_vec = np.concat([predecessor_vec,predecessors_vec,one_hot_term])\n",
    "  entry={\"predecessor\":predecessor_vec,\"predecessors\":predecessors_vec,\"term\":one_hot_term,\"chamber\":chamber,\"output\":output_vec}\n",
    "  # print(entry)\n",
    "  data.append(entry)\n",
    "\n",
    "# print(data[0])\n",
    "# inputs=np.stack(inputs)\n",
    "# outputs=np.stack(outputs)\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/input\",inputs[0])\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/output\",outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"outputs/prediction_vecs_08-04_charsim17.npz\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"outputs/prediction_vecs_08-04_charsim17.npz\",allow_pickle=True)[\"arr_0\"]\n",
    "display(data[0])\n",
    "# display(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "  def __init__(self,path):\n",
    "    data = np.load(path,allow_pickle=True)[\"arr_0\"]\n",
    "    self.inputs = [np.concatenate([entry[\"predecessor\"],entry[\"predecessors\"],entry[\"term\"],entry[\"chamber\"]]) for entry in data]\n",
    "    self.inputs = sklearn.preprocessing.StandardScaler().fit_transform(self.inputs)\n",
    "    self.outputs = [entry[\"output\"] for output in data]\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.inputs[idx],self.outputs[idx]\n",
    "\n",
    "ds = ActionDataset(\"outputs/prediction_vecs_08-04_charsim17.npz\")\n",
    "# print(len(ds))\n",
    "train_dataset,test_dataset,val_dataset = torch.utils.data.random_split(ds,[0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arr(arr):\n",
    "  for line in arr:\n",
    "    print([float(x) for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "# class ActionModel(torch.nn.Module):\n",
    "#   def __init__(self,input_len,output_len):\n",
    "#     self.linear= torch.nn.Linear\n",
    "#     self.input_len=input_len\n",
    "#     self.output_len = output_len\n",
    "#   def forward(self,input):\n",
    "\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+N_TERMS+2,len(common_bucket_names)+1)\n",
    "# model.load_state_dict(torch.load(\"outputs/models/08-04_lr1e-5_beta.3/epoch85.pt\"))\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "pred_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "beta=0.1\n",
    "folder = \"outputs/models/08-04_lr1e-5_beta.1\"\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i,(input,output) in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input=input.float()\n",
    "        output=output.float()\n",
    "        pred = model(input)\n",
    "        pred_loss = pred_loss_fn(pred,output)\n",
    "        ridge_loss = torch.sum(torch.abs(model.weight))\n",
    "        loss = pred_loss+beta*ridge_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"finished epoch {epoch}\")\n",
    "\n",
    "    torch.save({\"model\":model.state_dict(),\"optim\":optim.state_dict()},folder+f\"/epoch{epoch}.pt\")\n",
    "    with torch.no_grad():\n",
    "        val_pred_loss=torch.scalar_tensor(0)\n",
    "        val_ridge_loss=torch.sum(torch.abs(model.weight))\n",
    "        for input,output in val_loader:\n",
    "            input=input.float()\n",
    "            output=output.float()\n",
    "            pred = model(input)\n",
    "            # for line in output:\n",
    "            #   for i,x in enumerate(line):\n",
    "            #     if x>=0.9:\n",
    "            #       print(i)\n",
    "            #   print(torch.sum(line))\n",
    "            # break\n",
    "            # print_arr(output.numpy())\n",
    "            # print(\"sum:\",torch.sum(output))\n",
    "            val_pred_loss+=pred_loss_fn(pred,output)\n",
    "        val_pred_loss/=len(val_loader)\n",
    "    with open(folder+f\"/loss\",\"a\") as file:\n",
    "        file.write(f\"epoch {epoch}. val pred loss:{val_pred_loss}. val ridge loss:{val_ridge_loss}\")\n",
    "        file.write(\"\\n\")\n",
    "    print(\"val pred loss:\",val_pred_loss,\"val ridge loss:\",val_ridge_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.weight.detach().numpy()[:,len(common_bucket_names)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+N_TERMS+2,len(common_bucket_names)+1)\n",
    "model.load_state_dict(torch.load(\"outputs/models/08-04_lr1e-5_beta.1/epoch85.pt\"))\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "with torch.no_grad():\n",
    "  val_pred_loss=torch.scalar_tensor(0)\n",
    "  val_ridge_loss=torch.sum(torch.abs(model.weight))\n",
    "  for input,output in val_loader:\n",
    "    input=input.float()\n",
    "    output=output.float()\n",
    "    pred = model(input)\n",
    "    for line in pred:\n",
    "      print(list(line))\n",
    "      # for i,x in enumerate(line):\n",
    "      #   if x>=0.1:\n",
    "      #     print(i,float(x),end=\"\")\n",
    "      # print()\n",
    "      print(torch.sum(line))\n",
    "    # break\n",
    "    # print_arr(output.numpy())\n",
    "    # print(\"sum:\",torch.sum(output))\n",
    "  # val_pred_loss/=len(val_loader)\n",
    "  print(\"val pred loss:\",val_pred_loss,\"val ridge loss:\",val_ridge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "model.load_state_dict(torch.load(\"outputs/models/08-04_lr1e-5_beta.1/epoch85.pt\"))\n",
    "weights = model.weight.detach()\n",
    "weights=weights.numpy()\n",
    "predecessor_chains=[]\n",
    "predecessors_chains=[]\n",
    "term_chains=[]\n",
    "for i,bucket1 in enumerate(common_bucket_names):\n",
    "  for j,bucket2 in enumerate(common_bucket_names):\n",
    "    predecessor_chains.append((float(weights[j][i]),bucket1,bucket2))\n",
    "    predecessors_chains.append((float(weights[j][i+len(common_bucket_names)]),bucket1,bucket2))\n",
    "for term in range(N_TERMS):\n",
    "  for j,bucket2 in enumerate(common_bucket_names):\n",
    "    term_chains.append((float(weights[j][term+2*len(common_bucket_names)]),term*2+2009,bucket2))\n",
    "chamber_chains=[]\n",
    "for j,bucket in enumerate(common_bucket_names):\n",
    "    chamber_chains.append((float(weights[j][1+term+2*len(common_bucket_names)]),\"House\",bucket))\n",
    "for j,bucket in enumerate(common_bucket_names):\n",
    "    chamber_chains.append((float(weights[j][2+term+2*len(common_bucket_names)]),\"Senate\",bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in sorted(chamber_chains):\n",
    "  print(param[0],param[1],\"->\",param[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "import copy\n",
    "import random\n",
    "model = sklearn.linear_model.LogisticRegression(penalty=\"l1\",solver=\"saga\",multi_class=\"multinomial\")\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "data_shuffled = copy.deepcopy(data)\n",
    "random.shuffle(data_shuffled)\n",
    "inputs = [np.concatenate([e[\"predecessors\"],e[\"predecessor\"],e[\"term\"]]) for e in data_shuffled]\n",
    "outputs = [e[\"output\"] for e in data_shuffled]\n",
    "inputs= scaler.fit_transform(inputs)\n",
    "divider = int(len(data)*0.8)\n",
    "train_inputs,test_inputs = inputs[:divider], data_shuffled[divider:]\n",
    "train_outputs,test_outputs = outputs[:divider], outputs[divider:]\n",
    "model.fit(train_inputs,train_outputs)\n",
    "test_pred = model.predict_proba(test_inputs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billanalysis-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
