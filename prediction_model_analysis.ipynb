{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "with open(\"outputs/buckets_08-04_manual-llm-manual.json\",\"r\") as file:\n",
    "  buckets = json.load(file)\n",
    "with open(\"outputs/extra_buckets_08-04.json\",\"r\") as file:\n",
    "  extra_buckets = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = [f\"data/US/{term*2+2009-222}-{term*2+2010-222}_{term}th_Congress/csv/history.csv\" for term in range(111,120)]\n",
    "history_df_by_term=[pd.read_csv(ds) for ds in datasets]\n",
    "for i,df in enumerate(history_df_by_term):\n",
    "  df[\"term\"]=i+111\n",
    "history_df=pd.concat(history_df_by_term)\n",
    "\n",
    "\n",
    "bucket_map={}\n",
    "for name in buckets:\n",
    "  for action in buckets[name]:\n",
    "    bucket_map[action]=name\n",
    "extra_bucket_map=defaultdict(list)\n",
    "n_read_twice=0\n",
    "for name in extra_buckets:\n",
    "  for i,action in enumerate(extra_buckets[name]):\n",
    "    extra_bucket_map[action].append(name)\n",
    "bucket_lens=Counter()\n",
    "for action in history_df[\"action\"]:\n",
    "  bucket_lens[bucket_map[action]]+=1\n",
    "extra_bucket_lens=Counter()\n",
    "for i,action in enumerate(history_df[\"action\"]):\n",
    "  for extra_bucket in extra_bucket_map[action]:\n",
    "    extra_bucket_lens[extra_bucket]+=1\n",
    "common_bucket_names = [name for name in buckets.keys() if bucket_lens[name]>=50]\n",
    "common_extra_bucket_names = [name for name in extra_buckets.keys() if extra_bucket_lens[name]>=50]\n",
    "common_bucket_names_inv={name:i for i,name in enumerate(common_bucket_names)}\n",
    "common_extra_bucket_names_inv={name:i for i,name in enumerate(common_extra_bucket_names)}\n",
    "# with open(\"./outputs/bucket_names_output.jsonl\", \"r\") as file:\n",
    "#   for line in file:\n",
    "#     if line.strip():  # Skip empty lines\n",
    "#       response = json.loads(line)\n",
    "#       llm_buckets.append(response[\"response\"][\"body\"][\"output\"][0][\"content\"][0][\"text\"])\n",
    "      \n",
    "# history_df[\"llm_bucket\"] = history_df[\"bucket\"].map(dict(zip(bucket_names, llm_buckets)))\n",
    "# display(bucket_map)\n",
    "unbucketed=[]\n",
    "for action in history_df[\"action\"]:\n",
    "  if action not in bucket_map:\n",
    "    unbucketed.append(action)\n",
    "# print(len(unbucketed))\n",
    "# Add next bill_id column to compare\n",
    "history_df[\"bucket\"]=history_df[\"action\"].apply(lambda action:bucket_map[action])\n",
    "history_df[\"extra_buckets\"]=history_df[\"action\"].apply(lambda action:extra_bucket_map[action])\n",
    "bills={bill_id:group for (bill_id,group) in history_df.groupby(\"bill_id\")}\n",
    "bill_ids = list(bills.keys())\n",
    "\n",
    "MIN_TERM=min(history_df[\"term\"])\n",
    "MAX_TERM=max(history_df[\"term\"])\n",
    "N_TERMS=MAX_TERM-MIN_TERM+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(common_bucket_names))\n",
    "display(len(common_extra_bucket_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sum(extra_bucket_lens[name] for name in common_extra_bucket_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha=2/3\n",
    "data=[]\n",
    "\n",
    "bill_id=history_df.iloc[0][\"bill_id\"]\n",
    "predecessors_buckets=np.zeros(len(common_bucket_names))\n",
    "predecessor_buckets=np.zeros(len(common_bucket_names))\n",
    "predecessors_extra_buckets=np.zeros(len(common_extra_bucket_names))\n",
    "predecessor_extra_buckets=np.zeros(len(common_extra_bucket_names))\n",
    "curr_bucket = np.zeros(len(common_bucket_names))\n",
    "curr_extra_buckets = np.zeros(len(common_extra_bucket_names))\n",
    "inputs=[]\n",
    "outputs=[]\n",
    "n_predecessors_prev=0\n",
    "cum_extra_buckets=0\n",
    "for i,row in history_df.iterrows():\n",
    "  prev_bucket=np.array(curr_bucket)\n",
    "  prev_extra_buckets=np.array(curr_extra_buckets)\n",
    "  curr_bucket = np.zeros(len(common_bucket_names))\n",
    "  curr_extra_buckets = np.zeros(len(common_extra_bucket_names))\n",
    "  output_bucket = np.zeros(len(common_bucket_names)+1)\n",
    "  if row[\"bucket\"] in common_bucket_names_inv:\n",
    "    curr_bucket[common_bucket_names_inv[row.bucket]]=1\n",
    "    output_bucket[common_bucket_names_inv[row.bucket]]=1\n",
    "  else:\n",
    "    output_bucket[-1]=1\n",
    "  for extra_bucket in row[\"extra_buckets\"]:\n",
    "    # print(\"extra bucket:\",extra_bucket)\n",
    "    if extra_bucket in common_extra_bucket_names_inv:\n",
    "      curr_extra_buckets[common_extra_bucket_names_inv[extra_bucket]]=1\n",
    "  s=sum(curr_extra_buckets)\n",
    "  # print(s)\n",
    "  cum_extra_buckets+=s\n",
    "  \n",
    "  chamber = np.zeros(2)\n",
    "  if row[\"chamber\"]==\"House\":\n",
    "    chamber[0]=1\n",
    "  if row[\"chamber\"]==\"Senate\":\n",
    "    chamber[1]=1\n",
    "\n",
    "  # if sum(one_hot_prev)!=1 and sum(one_hot_prev)!=0:\n",
    "  #   print(\"non-0/1 sum\",i)\n",
    "  # n_predecessors=np.sum(predecessors_vec)\n",
    "  # if n_predecessors!=0 and n_predecessors!=n_predecessors_prev+1 and n_predecessors!=n_predecessors_prev:\n",
    "  #   print(n_predecessors,n_predecessors_prev,i)\n",
    "  # n_predecessors_prev=n_predecessors\n",
    "  if row[\"bill_id\"]!=bill_id:\n",
    "    bill_id=row[\"bill_id\"]\n",
    "    prev_bucket=np.zeros(len(common_bucket_names))\n",
    "    prev_extra_buckets=np.zeros(len(common_extra_bucket_names))\n",
    "    predecessors_buckets=np.zeros(len(common_bucket_names))\n",
    "    predecessor_buckets=np.zeros(len(common_bucket_names))\n",
    "    predecessors_extra_buckets=np.zeros(len(common_extra_bucket_names))\n",
    "    predecessor_extra_buckets=np.zeros(len(common_extra_bucket_names))\n",
    "  predecessor_buckets=(1-alpha)*predecessor_buckets+alpha*np.array(prev_bucket)\n",
    "  predecessors_buckets=np.array(predecessors_buckets)+prev_bucket\n",
    "  predecessor_extra_buckets=(1-alpha)*predecessor_extra_buckets+alpha*np.array(prev_extra_buckets)\n",
    "  predecessors_extra_buckets=np.array(predecessors_extra_buckets)+prev_extra_buckets\n",
    "  term=np.zeros(N_TERMS)\n",
    "  term[row.term-MIN_TERM]=1\n",
    "  # print(predecessor_bu/ckets)\n",
    "  # input_vec = np.concat([predecessor_vec,predecessors_vec,one_hot_term])\n",
    "  entry={\"predecessor_buckets\":predecessor_buckets,\"predecessors_buckets\":predecessors_buckets,\"predecessor_extra_buckets\":predecessor_extra_buckets,\"predecessors_extra_buckets\":predecessors_extra_buckets,\"term\":term,\"chamber\":chamber,\n",
    "         \"output_bucket\":output_bucket,\"output_extra_buckets\":curr_extra_buckets} # print(entry)\n",
    "  data.append(entry)\n",
    "\n",
    "print(cum_extra_buckets)\n",
    "# print(data[0])\n",
    "# inputs=np.stack(inputs)\n",
    "# outputs=np.stack(outputs)\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/input\",inputs[0])\n",
    "# np.savetxt(\"outputs/vectors/07-29_charsim17/output\",outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([entry for entry in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sum([sum(entry[\"output_extra_buckets\"]) for entry in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cum_extra_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cum_extra_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\"outputs/prediction_vecs_08-04_withextras.npz\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extra_bucket_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load(\"outputs/prediction_vecs_08-04_withextras.npz\",allow_pickle=True)[\"arr_0\"]\n",
    "display(data[1])\n",
    "# display(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([x for x in data if \"predecessor_buckets\" not in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sklearn\n",
    "\n",
    "class ActionDataset(Dataset):\n",
    "  def __init__(self,path):\n",
    "    data = np.load(path,allow_pickle=True)[\"arr_0\"]\n",
    "    self.inputs = [np.concatenate([entry[\"predecessor_buckets\"],entry[\"predecessors_buckets\"],entry[\"predecessor_extra_buckets\"],entry[\"predecessors_extra_buckets\"],entry[\"term\"],entry[\"chamber\"]]) for entry in data]\n",
    "    self.inputs = sklearn.preprocessing.StandardScaler().fit_transform(self.inputs)\n",
    "    self.outputs = [np.concatenate((entry[\"output_bucket\"],entry[\"output_extra_buckets\"])) for entry in data]\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.inputs[idx],self.outputs[idx]\n",
    "\n",
    "ds = ActionDataset(\"outputs/prediction_vecs_08-04_withextras.npz\")\n",
    "# print(len(ds))\n",
    "train_dataset,test_dataset,val_dataset = torch.utils.data.random_split(ds,[0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_arr(arr):\n",
    "  for line in arr:\n",
    "    print([float(x) for x in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "# class ActionModel(torch.nn.Module):\n",
    "#   def __init__(self,input_len,output_len):\n",
    "#     self.linear= torch.nn.Linear\n",
    "#     self.input_len=input_len\n",
    "#     self.output_len = output_len\n",
    "#   def forward(self,input):\n",
    "\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+len(common_extra_bucket_names)*2+N_TERMS+2,len(common_bucket_names)+1+len(common_extra_bucket_names))\n",
    "# model.load_state_dict(torch.load(\"outputs/models/08-04_lr1e-5_beta.3/epoch85.pt\"))\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-5)\n",
    "pred_loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "beta=1e-4\n",
    "folder = \"outputs/models/08-04_withextras_bce_lr1e-5_beta1e-4\"\n",
    "if not os.path.isdir(folder):\n",
    "    os.mkdir(folder)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for i,(input,output) in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        input=input.float()\n",
    "        output=output.float()\n",
    "        pred = model(input)\n",
    "        pred_loss = pred_loss_fn(pred,output)\n",
    "        # print(pred_loss.requires_grad)\n",
    "        ridge_loss = torch.norm(model.weight,p=1)\n",
    "        loss = pred_loss+beta*ridge_loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"finished epoch {epoch}\")\n",
    "    if epoch<5 or (epoch%5==0 and epoch<=20) or epoch%10==0:\n",
    "        torch.save({\"model\":model.state_dict(),\"optim\":optim.state_dict()},folder+f\"/epoch{epoch}.pt\")\n",
    "    with torch.no_grad():\n",
    "        val_pred_loss=torch.scalar_tensor(0)\n",
    "        val_ridge_loss=torch.sum(torch.abs(model.weight))\n",
    "        for input,output in val_loader:\n",
    "            input=input.float()\n",
    "            output=output.float()\n",
    "            pred = model(input)\n",
    "            # for line in output:\n",
    "            #   for i,x in enumerate(line):\n",
    "            #     if x>=0.9:\n",
    "            #       print(i)\n",
    "            #   print(torch.sum(line))\n",
    "            # break\n",
    "            # print_arr(output.numpy())\n",
    "            # print(\"sum:\",torch.sum(output))\n",
    "            val_pred_loss+=pred_loss_fn(pred,output)\n",
    "        val_pred_loss/=len(val_loader)\n",
    "    print(\"log val pred loss:\",torch.log10(val_pred_loss),\"val ridge loss:\",val_ridge_loss)\n",
    "    with open(folder+f\"/loss\",\"a\") as file:\n",
    "        file.write(f\"epoch {epoch}. val pred loss:{val_pred_loss}. val ridge loss:{val_ridge_loss}\")\n",
    "        file.write(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+len(common_extra_bucket_names)*2+N_TERMS+2,len(common_bucket_names)+1+len(common_extra_bucket_names))\n",
    "model.load_state_dict(torch.load(\"outputs/models/08-04_withextras_bce_lr1e-5_beta1e-4/epoch200.pt\")[\"model\"])\n",
    "val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset,batch_size=32,shuffle=True)\n",
    "import itertools\n",
    "def argmax(l):\n",
    "\n",
    "  return l.index(max(l))\n",
    "    \n",
    "with torch.no_grad():\n",
    "  val_pred_loss=torch.scalar_tensor(0)\n",
    "  val_ridge_loss=torch.sum(torch.abs(model.weight))\n",
    "  eps=1e-2\n",
    "  for input,output in val_loader:\n",
    "    input=input.float()\n",
    "    output=output.float()\n",
    "    pred = model(input)\n",
    "    for input_line,output_line,pred_line in zip(input,output,pred):\n",
    "      # print(len(common_bucket_names))\n",
    "      # print(line[-1])\n",
    "      # line=list(line)\n",
    "      # output_dict = {name:torch.sigmoid(logodds) for name,logodds in zip(common_bucket_names+[\"No Bucket\"]+common_extra_bucket_names,pred_line)}\n",
    "      # if torch.sum(output_line[-len(common_extra_bucket_names):])>0:\n",
    "      # print(list(output_line).index(1))\n",
    "        # errors = sorted([((float(torch.sigmoid(logodds)-output_p)),float(output_p),name) for name,output_p,logodds in zip(common_bucket_names+[\"No Bucket\"]+[\"Extra bucket: \"+x for x in common_extra_bucket_names],output_line,pred_line) if torch.abs(output_p-1)<eps],reverse=True)\n",
    "      # print([float(x) for x in output_line[-len(common_extra_bucket_names):]])\n",
    "      # print(output_line[:-len(common_extra_bucket_names)])\n",
    "      # break\n",
    "        # display(errors)\n",
    "      \n",
    "    # break# for i,x in enumerate(line):\n",
    "      #   if x>=0.1:\n",
    "      #     print(i,float(x),end=\"\")\n",
    "      # print()\n",
    "      # print(torch.sum(line))\n",
    "    # break\n",
    "    # print_arr(output.numpy())\n",
    "    # print(\"sum:\",torch.sum(output))\n",
    "  # val_pred_loss/=len(val_loader)\n",
    "  # print(\"val pred loss:\",val_pred_loss,\"val ridge loss:\",val_ridge_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_bucket_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import itertools\n",
    "# model.load_state_dict(torch.load(\"outputs/models/08-04_lr1e-5_beta.01/epoch99.pt\")[\"model\"])\n",
    "model = torch.nn.Linear(len(common_bucket_names)*2+len(common_extra_bucket_names)*2+N_TERMS+2,len(common_bucket_names)+1+len(common_extra_bucket_names))\n",
    "model.load_state_dict(torch.load(\"outputs/models/08-04_withextras_bce_lr1e-5_beta1e-06/epoch200.pt\")[\"model\"])\n",
    "\n",
    "weights = model.weight.detach().numpy()\n",
    "predecessor_chains=[]\n",
    "predecessors_chains=[]\n",
    "predecessor_s_chains=[]\n",
    "predecessor_plus_s_chains=[]\n",
    "predecessor_no_s_chains=[]\n",
    "term_chains=[]\n",
    "common_extra_bucket_names_prefixed=[\"Extra Bucket: \"+x for x in common_extra_bucket_names]\n",
    "for i,bucket1 in enumerate(common_bucket_names):\n",
    "  for j,bucket2 in enumerate(itertools.chain(common_bucket_names,common_extra_bucket_names_prefixed)):\n",
    "    # print(i)\n",
    "    predecessor_chains.append((float(weights[j][i]),bucket1,bucket2))\n",
    "    predecessors_chains.append((float(weights[j][i+len(common_bucket_names)]),bucket1,bucket2))\n",
    "    predecessor_plus_s_chains.append((predecessors_chains[-1][0]+predecessor_chains[-1][0],bucket1,bucket2))\n",
    "    predecessor_no_s_chains.append((predecessor_chains[-1][0]-predecessors_chains[-1][0],bucket1,bucket2))\n",
    "    predecessor_s_chains.append((predecessors_chains[-1][0]-predecessor_chains[-1][0],bucket1,bucket2))\n",
    "for i,bucket1 in enumerate(common_extra_bucket_names_prefixed):\n",
    "  for j,bucket2 in enumerate(common_bucket_names+common_extra_bucket_names_prefixed):\n",
    "    predecessor_chains.append((float(weights[j][i+2*len(common_bucket_names)]),bucket1,bucket2))\n",
    "    predecessors_chains.append((float(weights[j][i+2*len(common_bucket_names)+len(common_extra_bucket_names)]),bucket1,bucket2))\n",
    "    predecessor_plus_s_chains.append((predecessors_chains[-1][0]+predecessor_chains[-1][0],bucket1,bucket2))\n",
    "    predecessor_no_s_chains.append((predecessor_chains[-1][0]-predecessors_chains[-1][0],bucket1,bucket2))\n",
    "    predecessor_s_chains.append((predecessors_chains[-1][0]-predecessor_chains[-1][0],bucket1,bucket2))\n",
    "for term in range(N_TERMS):\n",
    "  for j,bucket2 in enumerate(itertools.chain(common_bucket_names,common_extra_bucket_names_prefixed)):\n",
    "    term_chains.append((float(weights[j][term+2*len(common_bucket_names)+2*len(common_extra_bucket_names)]),str(term*2+2009)+\"-\"+str(term*2+2010),bucket2))\n",
    "chamber_chains=[]\n",
    "for j,bucket in enumerate(itertools.chain(common_bucket_names,common_extra_bucket_names_prefixed)):\n",
    "    chamber_chains.append((float(weights[j][1+term+2*len(common_bucket_names)+2*len(common_extra_bucket_names)]),\"House\",bucket))\n",
    "for j,bucket in enumerate(itertools.chain(common_bucket_names,common_extra_bucket_names_prefixed)):\n",
    "    chamber_chains.append((float(weights[j][2+term+2*len(common_bucket_names)+2*len(common_extra_bucket_names)]),\"Senate\",bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def bucket_by_magnitude(chains):\n",
    "  return Counter([np.round(np.log(abs(x[0]))) for x in chains])\n",
    "\n",
    "# display(predecessor_effects)\n",
    "term_effects = bucket_by_magnitude(predecessor_no_s_chains)\n",
    "plt.bar(term_effects.keys(),term_effects.values())\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predecessor_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_chains(chains):\n",
    "  display([str(param[0])+\" \"+param[1]+\" -> \"+param[2] for param in sorted(chains,key=lambda x:abs(x[0]),reverse=True) if param[1]!=param[2] or param[0]>0][:100])\n",
    "  print(sum(x[0] for x in sorted(chains,key=lambda x: abs(x[0]),reverse=True)[-10:])/10)\n",
    "display_chains(predecessor_chains)\n",
    "# for param in sorted(predecessor_s_chains,key=lambda x: abs(x[0]),reverse=True)[:100]:\n",
    "#   print(param[0],param[1],\"->\",param[2])\n",
    "# for param in sorted(predecessor_chains,key=lambda x: abs(x[0]),reverse=True)[-100:]:\n",
    "#   print(param[0],param[1],\"->\",param[2])\n",
    "# for param in sorted(predecessor_chains[-100:],key=lambda x: abs(x[0]),reverse=True):\n",
    "#   print(param[0],param[1],\"->\",param[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "import sklearn\n",
    "import copy\n",
    "import random\n",
    "model = sklearn.linear_model.LogisticRegression(penalty=\"l1\",solver=\"saga\",multi_class=\"multinomial\")\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "data_shuffled = copy.deepcopy(data)\n",
    "random.shuffle(data_shuffled)\n",
    "inputs = [np.concatenate([e[\"predecessors\"],e[\"predecessor\"],e[\"term\"]]) for e in data_shuffled]\n",
    "outputs = [e[\"output\"] for e in data_shuffled]\n",
    "inputs= scaler.fit_transform(inputs)\n",
    "divider = int(len(data)*0.8)\n",
    "train_inputs,test_inputs = inputs[:divider], data_shuffled[divider:]\n",
    "train_outputs,test_outputs = outputs[:divider], outputs[divider:]\n",
    "model.fit(train_inputs,train_outputs)\n",
    "test_pred = model.predict_proba(test_inputs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billanalysis-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
